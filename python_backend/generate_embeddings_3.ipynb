{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58452185-afed-4089-901a-97d73201ce8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:33:36.107127Z",
     "iopub.status.busy": "2025-05-17T22:33:36.105923Z",
     "iopub.status.idle": "2025-05-17T22:33:43.242008Z",
     "shell.execute_reply": "2025-05-17T22:33:43.240681Z",
     "shell.execute_reply.started": "2025-05-17T22:33:36.107086Z"
    },
    "id": "58452185-afed-4089-901a-97d73201ce8c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/jupyter/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74372ac9-f875-49ed-b1e2-47a1bb302dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:33:43.245464Z",
     "iopub.status.busy": "2025-05-17T22:33:43.244360Z",
     "iopub.status.idle": "2025-05-17T22:33:46.484911Z",
     "shell.execute_reply": "2025-05-17T22:33:46.483612Z",
     "shell.execute_reply.started": "2025-05-17T22:33:43.245407Z"
    },
    "id": "74372ac9-f875-49ed-b1e2-47a1bb302dc7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wy1znS7Xwc9j",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:33:46.488038Z",
     "iopub.status.busy": "2025-05-17T22:33:46.486851Z",
     "iopub.status.idle": "2025-05-17T22:33:49.764741Z",
     "shell.execute_reply": "2025-05-17T22:33:49.763374Z",
     "shell.execute_reply.started": "2025-05-17T22:33:46.487974Z"
    },
    "id": "wy1znS7Xwc9j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: umap-learn in /home/jupyter/.local/lib/python3.10/site-packages (0.5.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/jupyter/.local/lib/python3.10/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (65.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7G4W8pU85yI",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:34:06.569730Z",
     "iopub.status.busy": "2025-05-17T22:34:06.568562Z",
     "iopub.status.idle": "2025-05-17T22:35:32.601517Z",
     "shell.execute_reply": "2025-05-17T22:35:32.600264Z",
     "shell.execute_reply.started": "2025-05-17T22:34:06.569692Z"
    },
    "id": "f7G4W8pU85yI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-05-17 22:34:53.026638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-17 22:35:01.273072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, random, pathlib, textwrap, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.cluster import (KMeans, DBSCAN, Birch, AgglomerativeClustering,\n",
    "                             OPTICS, SpectralClustering)\n",
    "\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from hdbscan.validity import validity_index\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "jQUdqKBM89_y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2025-05-17T22:35:43.195697Z",
     "iopub.status.busy": "2025-05-17T22:35:43.194379Z",
     "iopub.status.idle": "2025-05-17T22:35:47.728708Z",
     "shell.execute_reply": "2025-05-17T22:35:47.727110Z",
     "shell.execute_reply.started": "2025-05-17T22:35:43.195643Z"
    },
    "id": "jQUdqKBM89_y",
    "outputId": "5c452037-41cd-45df-936d-e244f6666d33",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>error_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Building for target x86_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Building for target x86_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Hunk #1 succeeded at 129 (offset 12 lines).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Hunk #2 succeeded at 166 with fuzz 1 (offset 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Hunk #3 succeeded at 299 with fuzz 1 (offset 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49593</th>\n",
       "      <td>321</td>\n",
       "      <td>error: Bad exit status from /usr/src/tmp/rpm-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49594</th>\n",
       "      <td>321</td>\n",
       "      <td>Bad exit status from /usr/src/tmp/rpm-tmp.7673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49595</th>\n",
       "      <td>321</td>\n",
       "      <td>Command exited with non-zero status 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49596</th>\n",
       "      <td>321</td>\n",
       "      <td>hsh-rebuild: rebuild of `libprojectM-2.1.0-alt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49597</th>\n",
       "      <td>321</td>\n",
       "      <td>Command exited with non-zero status 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49598 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id                                         error_text\n",
       "0           0                         Building for target x86_64\n",
       "1           0                         Building for target x86_64\n",
       "2           0        Hunk #1 succeeded at 129 (offset 12 lines).\n",
       "3           0  Hunk #2 succeeded at 166 with fuzz 1 (offset 1...\n",
       "4           0  Hunk #3 succeeded at 299 with fuzz 1 (offset 1...\n",
       "...       ...                                                ...\n",
       "49593     321  error: Bad exit status from /usr/src/tmp/rpm-t...\n",
       "49594     321  Bad exit status from /usr/src/tmp/rpm-tmp.7673...\n",
       "49595     321              Command exited with non-zero status 1\n",
       "49596     321  hsh-rebuild: rebuild of `libprojectM-2.1.0-alt...\n",
       "49597     321              Command exited with non-zero status 1\n",
       "\n",
       "[49598 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"combined_logs_with_labels.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['id'] = df.index\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "MIN_LENGTH = 20\n",
    "MIN_LETTERS = 10\n",
    "MIN_PRINTABLE_RATIO = 0.7\n",
    "MAX_SPECIAL_RATIO = 0.1\n",
    "\n",
    "def is_valid_error_line(line: str) -> bool:\n",
    "    line = line.strip()\n",
    "    if len(line) < MIN_LENGTH:\n",
    "        return False\n",
    "\n",
    "    letter_count = sum(c.isalpha() for c in line)\n",
    "    if letter_count < MIN_LETTERS:\n",
    "        return False\n",
    "\n",
    "    printable_count = sum(c.isprintable() for c in line)\n",
    "    if printable_count / len(line) < MIN_PRINTABLE_RATIO:\n",
    "        return False\n",
    "\n",
    "    special_chars = re.findall(r'[^\\w\\s\\.\\-_:;,?!@#\\$%^&*()+=]', line)\n",
    "    if len(special_chars) / len(line) > MAX_SPECIAL_RATIO:\n",
    "        return False\n",
    "\n",
    "    if re.search(r'(\\\\x[0-9a-fA-F]{2}|�|\\u0000|\\uFFFD)', line):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def process_errors(input_path: str, output_path: Optional[str] = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(input_path)\n",
    "    df['id'] = df.index\n",
    "    processed_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row['errors']):\n",
    "            for error_line in row['errors'].split('\\n'):\n",
    "                line = error_line.strip()\n",
    "                if is_valid_error_line(line):\n",
    "                    processed_data.append({\n",
    "                        'log_id': idx,\n",
    "                        'error_text': line\n",
    "                    })\n",
    "\n",
    "    result_df = pd.DataFrame(processed_data)\n",
    "    if output_path:\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "    return result_df\n",
    "\n",
    "PROCESSED_PATH = \"processed_errors_clean.csv\"\n",
    "process_errors(DATA_PATH, PROCESSED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nq1aJEbtValA",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:35:56.149466Z",
     "iopub.status.busy": "2025-05-17T22:35:56.148519Z",
     "iopub.status.idle": "2025-05-17T22:35:56.245487Z",
     "shell.execute_reply": "2025-05-17T22:35:56.244235Z",
     "shell.execute_reply.started": "2025-05-17T22:35:56.149430Z"
    },
    "id": "nq1aJEbtValA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv(PROCESSED_PATH)\n",
    "errors_texts = processed_df['error_text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "S32l9adj8_fT",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:35:59.031489Z",
     "iopub.status.busy": "2025-05-17T22:35:59.030234Z",
     "iopub.status.idle": "2025-05-17T22:35:59.048104Z",
     "shell.execute_reply": "2025-05-17T22:35:59.046979Z",
     "shell.execute_reply.started": "2025-05-17T22:35:59.031453Z"
    },
    "id": "S32l9adj8_fT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMB_DIR = pathlib.Path(\"embeddings\"); EMB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODELS = {\n",
    "    \"distil-log\": \"teoogherghi/Log-Analysis-Model-DistilBert\",\n",
    "    \"roberta-large\": \"roberta-large\",\n",
    "    \"deberta-v3-large\": \"microsoft/deberta-v3-large\",\n",
    "    \"longformer\": \"allenai/longformer-base-4096\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2EAZ0DDa9BAs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:36:02.576636Z",
     "iopub.status.busy": "2025-05-17T22:36:02.575220Z",
     "iopub.status.idle": "2025-05-17T22:36:02.815897Z",
     "shell.execute_reply": "2025-05-17T22:36:02.814687Z",
     "shell.execute_reply.started": "2025-05-17T22:36:02.576580Z"
    },
    "id": "2EAZ0DDa9BAs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2uyrQU-s9Cd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:36:05.279099Z",
     "iopub.status.busy": "2025-05-17T22:36:05.277835Z",
     "iopub.status.idle": "2025-05-17T22:36:05.313562Z",
     "shell.execute_reply": "2025-05-17T22:36:05.312403Z",
     "shell.execute_reply.started": "2025-05-17T22:36:05.279054Z"
    },
    "id": "2uyrQU-s9Cd1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_st(model_name: str, texts, batch_size=BATCH):\n",
    "    encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "    emb = encoder.encode(texts, batch_size=batch_size,\n",
    "                         show_progress_bar=True,\n",
    "                         convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return emb.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Wo789rNf9Dxs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T22:36:07.877745Z",
     "iopub.status.busy": "2025-05-17T22:36:07.876240Z",
     "iopub.status.idle": "2025-05-17T22:36:07.964314Z",
     "shell.execute_reply": "2025-05-17T22:36:07.963071Z",
     "shell.execute_reply.started": "2025-05-17T22:36:07.877658Z"
    },
    "id": "Wo789rNf9Dxs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    RobertaTokenizer,\n",
    "    DebertaV2Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9iJNQg8x9EgI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-17T22:36:12.240356Z",
     "iopub.status.busy": "2025-05-17T22:36:12.239079Z",
     "iopub.status.idle": "2025-05-17T23:13:43.213789Z",
     "shell.execute_reply": "2025-05-17T23:13:43.212504Z",
     "shell.execute_reply.started": "2025-05-17T22:36:12.240285Z"
    },
    "id": "9iJNQg8x9EgI",
    "outputId": "3ab955f2-1234-445a-db3c-fc3a34a4825d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ distil-log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1550/1550 [02:09<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1550/1550 [02:19<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ deberta-v3-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1550/1550 [07:28<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ longformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "100%|██████████| 1550/1550 [19:19<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for key, hub_id in MODELS.items():\n",
    "    print(f\"⏳ {key}\")\n",
    "    try:\n",
    "        if key == \"distil-log\":\n",
    "            from transformers import DistilBertModel, DistilBertTokenizer\n",
    "            tok = DistilBertTokenizer.from_pretrained(hub_id)\n",
    "            mdl = DistilBertModel.from_pretrained(hub_id).to(DEVICE).eval()\n",
    "            tokenizer_kwargs = {\n",
    "                'truncation': True,\n",
    "                'max_length': 256,\n",
    "                'padding': 'max_length',\n",
    "                'return_tensors': \"pt\"\n",
    "            }\n",
    "\n",
    "        elif key == \"longformer\":\n",
    "            tok = AutoTokenizer.from_pretrained(hub_id)\n",
    "            mdl = AutoModel.from_pretrained(hub_id, attention_window=256).to(DEVICE).eval()\n",
    "            tokenizer_kwargs = {\n",
    "                'truncation': True,\n",
    "                'max_length': 1024,\n",
    "                'padding': 'max_length',\n",
    "                'return_tensors': \"pt\"\n",
    "            }\n",
    "\n",
    "        elif key == \"deberta-v3-large\":\n",
    "            tok = DebertaV2Tokenizer.from_pretrained(hub_id)\n",
    "            mdl = AutoModel.from_pretrained(hub_id).to(DEVICE).eval()\n",
    "            tokenizer_kwargs = {\n",
    "                'truncation': True,\n",
    "                'max_length': 128,\n",
    "                'padding': 'max_length',\n",
    "                'return_tensors': \"pt\"\n",
    "            }\n",
    "\n",
    "        elif key == \"roberta-large\":\n",
    "            tok = RobertaTokenizer.from_pretrained(hub_id)\n",
    "            mdl = AutoModel.from_pretrained(hub_id).to(DEVICE).eval()\n",
    "            tokenizer_kwargs = {\n",
    "                'truncation': True,\n",
    "                'max_length': 128,\n",
    "                'padding': True,\n",
    "                'return_tensors': \"pt\"\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            emb = encode_st(hub_id, errors_texts)\n",
    "            #pd.DataFrame(emb, index=df.id).to_csv(EMB_DIR / f\"{key}.csv\", index_label=\"id\")\n",
    "            pd.DataFrame(emb, index=processed_df.log_id).to_csv(EMB_DIR / f\"{key}.csv\", index_label=\"id\")\n",
    "            continue\n",
    "\n",
    "        all_vecs = []\n",
    "        for i in tqdm(range(0, len(errors_texts), BATCH)):\n",
    "            batch = errors_texts[i:i+BATCH]\n",
    "            inputs = tok(batch, **tokenizer_kwargs).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outs = mdl(**inputs).last_hidden_state.mean(1)\n",
    "            all_vecs.append(outs.cpu())\n",
    "\n",
    "        emb = torch.cat(all_vecs, 0).numpy()\n",
    "        emb = normalize(emb)\n",
    "        #pd.DataFrame(emb, index=df.id).to_csv(EMB_DIR / f\"{key}.csv\", index_label=\"id\")\n",
    "        pd.DataFrame(emb, index=processed_df.log_id).to_csv(EMB_DIR / f\"{key}.csv\", index_label=\"id\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Critical error with {key}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Ue2mxSUO9Iym",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:16:50.591202Z",
     "iopub.status.busy": "2025-05-17T23:16:50.590237Z",
     "iopub.status.idle": "2025-05-17T23:16:50.611741Z",
     "shell.execute_reply": "2025-05-17T23:16:50.610622Z",
     "shell.execute_reply.started": "2025-05-17T23:16:50.591146Z"
    },
    "id": "Ue2mxSUO9Iym",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLUSTER_DIR = pathlib.Path(\"clusters\"); CLUSTER_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0kRft9Tr9LKM",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:16:57.502857Z",
     "iopub.status.busy": "2025-05-17T23:16:57.501518Z",
     "iopub.status.idle": "2025-05-17T23:16:57.528960Z",
     "shell.execute_reply": "2025-05-17T23:16:57.527925Z",
     "shell.execute_reply.started": "2025-05-17T23:16:57.502822Z"
    },
    "id": "0kRft9Tr9LKM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALGORITHMS = {\n",
    "    \"kmeans\": {\n",
    "        \"func\": lambda X, params: KMeans(**params).fit(X).labels_,\n",
    "        \"params\": [\n",
    "            {\"n_clusters\": 20, \"n_init\": \"auto\"},\n",
    "            {\"n_clusters\": 30, \"n_init\": 10},\n",
    "            {\"n_clusters\": 5, \"n_init\": 10}\n",
    "        ]\n",
    "    },\n",
    "    \"agglo\": {\n",
    "        \"func\": lambda X, params: AgglomerativeClustering(**params).fit_predict(X),\n",
    "        \"params\": [\n",
    "            {\"n_clusters\": 20, \"linkage\": \"ward\"},\n",
    "            {\"n_clusters\": 15, \"linkage\": \"complete\"}\n",
    "        ]\n",
    "    },\n",
    "    \"dbscan\": {\n",
    "        \"func\": lambda X, params: DBSCAN(**params).fit_predict(X),\n",
    "        \"params\": [\n",
    "            {\"eps\": 0.3, \"min_samples\": 10, \"metric\": \"euclidean\"},\n",
    "            {\"eps\": 0.5, \"min_samples\": 15, \"metric\": \"cosine\"}\n",
    "        ]\n",
    "    },\n",
    "    \"hdbscan\": {\n",
    "        \"func\": lambda X, params: hdbscan.HDBSCAN(**params).fit_predict(X),\n",
    "        \"params\": [\n",
    "            {\"min_cluster_size\": 15, \"cluster_selection_epsilon\": 0.3},\n",
    "            {\"min_cluster_size\": 10, \"cluster_selection_method\": \"leaf\"},\n",
    "            {\"min_cluster_size\": 20, \"min_samples\": 5}\n",
    "        ]\n",
    "    },\n",
    "    \"umap_hdbscan\": {\n",
    "        \"func\": lambda X, params: hdbscan.HDBSCAN(**params).fit_predict(\n",
    "            UMAP(n_components=50, random_state=42).fit_transform(X)),\n",
    "        \"params\": [{\"min_cluster_size\": 10}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ae159-1530-44fd-a8eb-7dc4cbae3893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:17:02.083222Z",
     "iopub.status.busy": "2025-05-17T23:17:02.082101Z"
    },
    "id": "cd5ae159-1530-44fd-a8eb-7dc4cbae3893",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== deberta-v3-large ===\n",
      "⚠️ kmeans failed: array length 49598 does not match index length 2140\n",
      "⚠️ kmeans failed: array length 49598 does not match index length 2140\n",
      "⚠️ kmeans failed: array length 49598 does not match index length 2140\n",
      "⚠️ agglo failed: array length 49598 does not match index length 2140\n"
     ]
    }
   ],
   "source": [
    "def cluster_and_report(name, X, algo_key, params):\n",
    "    algo_func = ALGORITHMS[algo_key][\"func\"]\n",
    "    try:\n",
    "        labels = algo_func(X, params)\n",
    "        n_clusters = len(set(labels))\n",
    "\n",
    "        stats = {\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"noise_ratio\": np.mean(labels == -1),\n",
    "            \"silhouette\": silhouette_score(X, labels) if n_clusters > 1 else np.nan,\n",
    "            \"davies_bouldin\": davies_bouldin_score(X, labels),\n",
    "            \"calinski_harabasz\": calinski_harabasz_score(X, labels),\n",
    "            \"cluster_sizes\": np.bincount(labels[labels != -1]).tolist()\n",
    "        }\n",
    "\n",
    "        if algo_key.startswith(\"hdbscan\"):\n",
    "            stats[\"DBCV\"] = validity_index(X, labels)\n",
    "\n",
    "        #label_series = pd.Series(labels, index=df.id, name=\"cluster\")\n",
    "        label_series = pd.Series(labels, index=processed_df.log_id, name=\"cluster\")\n",
    "        label_series.to_csv(CLUSTER_DIR / f\"{name}_{algo_key}_{param_str(params)}_labels.csv\", index_label=\"id\")\n",
    "\n",
    "        sample_df = pd.DataFrame({'text': df['errors'], 'cluster': labels})\n",
    "        sample_df.groupby('cluster').apply(lambda x: x.sample(3)).to_csv(\n",
    "        CLUSTER_DIR / f\"{name}_samples.csv\")\n",
    "\n",
    "        return {**stats, \"params\": params}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ {algo_key} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def param_str(params):\n",
    "    return \"_\".join(f\"{k}={v}\" for k,v in params.items())\n",
    "\n",
    "# %%\n",
    "results = []\n",
    "\n",
    "for emb_file in sorted(EMB_DIR.glob(\"*.csv\")):\n",
    "    name = emb_file.stem\n",
    "    X = pd.read_csv(emb_file, index_col=\"id\").values\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    for algo_key in ALGORITHMS:\n",
    "        for params in ALGORITHMS[algo_key][\"params\"]:\n",
    "            result = cluster_and_report(name, X, algo_key, params)\n",
    "            if result:\n",
    "                results.append({\n",
    "                    \"embedding\": name,\n",
    "                    \"algorithm\": algo_key,\n",
    "                    **result\n",
    "                })\n",
    "                print(f\"{algo_key:8s} [params: {params}] → clusters={result['n_clusters']}\")\n",
    "\n",
    "# %%\n",
    "CENTRAL_DIR = pathlib.Path(\"cluster_objects\"); CENTRAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def central_extremes(X, labels, top_k=10):\n",
    "    out = defaultdict(lambda: {\"central\": [], \"extreme\": []})\n",
    "    for c in set(labels):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        part = X[idx]\n",
    "        center = part.mean(0, keepdims=True)\n",
    "        dists = np.linalg.norm(part - center, axis=1)\n",
    "        order = np.argsort(dists)\n",
    "        out[c][\"central\"] = idx[order[:top_k]].tolist()\n",
    "        out[c][\"extreme\"] = idx[order[::-1][:top_k]].tolist()\n",
    "    return out\n",
    "\n",
    "# %%\n",
    "for lab_file in sorted(CLUSTER_DIR.glob(\"*_labels.csv\")):\n",
    "    parts = lab_file.stem.split(\"_\")\n",
    "    name, algo = parts[0], \"_\".join(parts[1:-1])\n",
    "    labels = pd.read_csv(lab_file, index_col=\"id\").squeeze().values\n",
    "    X = pd.read_csv(EMB_DIR / f\"{name}.csv\", index_col=\"id\").values\n",
    "    ce = central_extremes(X, labels)\n",
    "\n",
    "    md_path = CENTRAL_DIR / f\"{lab_file.stem}.md\"\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(f\"# {name} + {algo}\\n\\n\")\n",
    "        fp.write(f\"## Cluster Statistics\\n\")\n",
    "        fp.write(f\"- Total clusters: {len(ce)}\\n\")\n",
    "        fp.write(f\"- Noise points: {np.sum(labels == -1)}\\n\\n\")\n",
    "\n",
    "        for c, data in ce.items():\n",
    "            fp.write(f\"## Cluster {c} (Size: {len(data['central']) + len(data['extreme'])})\\n\\n\")\n",
    "            fp.write(\"### Central objects\\n\")\n",
    "            for ix in data[\"central\"]:\n",
    "                row = df.loc[ix]\n",
    "                fp.write(f\"* **id={ix}** — `{row.errors}`\\n\")  # Полный текст\n",
    "            fp.write(\"\\n### Extreme objects\\n\")\n",
    "            for ix in data[\"extreme\"]:\n",
    "                row = df.loc[ix]\n",
    "                fp.write(f\"* **id={ix}** — `{row.errors}`\\n\")  # Полный текст\n",
    "            fp.write(\"\\n---\\n\")\n",
    "    print(\"✓\", md_path)\n",
    "\n",
    "# %%\n",
    "VIS_DIR = pathlib.Path(\"viz\"); VIS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def umap_plot(name, algo, X, labels):\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42,\n",
    "                       n_neighbors=15, min_dist=0.1, metric='cosine')\n",
    "    emb2d = reducer.fit_transform(X)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    scatter = plt.scatter(emb2d[:,0], emb2d[:,1], s=10, c=labels,\n",
    "                         cmap='tab20', alpha=0.8)\n",
    "    plt.title(f\"{name} + {algo}\\nClusters: {len(set(labels))}\")\n",
    "    plt.colorbar(scatter)\n",
    "    path = VIS_DIR / f\"{name}_{algo}.png\"\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "    return path\n",
    "def plot_cluster_distribution(labels):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sizes = np.bincount(labels[labels != -1])\n",
    "    plt.bar(range(len(sizes)), sizes)\n",
    "    plt.title(f\"Cluster Size Distribution (Total: {len(sizes)})\")\n",
    "    plt.xlabel(\"Cluster ID\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.savefig(VIS_DIR / \"cluster_distribution.png\")\n",
    "    plt.close()\n",
    "# %%\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df = res_df.explode(\"params\").reset_index(drop=True)\n",
    "res_df.to_markdown(\"full_report.md\", index=False)\n",
    "res_df.to_csv(\"full_report.csv\", index=False)\n",
    "res_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3601479b-77c7-4942-a6eb-754e38aa9f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
