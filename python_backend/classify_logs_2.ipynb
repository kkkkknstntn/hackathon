{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825d61ed-5f9a-4ab1-9835-4e534a4fae48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:04:28.221966Z",
     "iopub.status.busy": "2025-05-16T19:04:28.220061Z",
     "iopub.status.idle": "2025-05-16T19:04:28.261081Z",
     "shell.execute_reply": "2025-05-16T19:04:28.259177Z",
     "shell.execute_reply.started": "2025-05-16T19:04:28.221924Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install torch torchaudio transformers diffusers torchvision --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0b6291-ae66-4e25-b13c-76487802848d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:04:28.265908Z",
     "iopub.status.busy": "2025-05-16T19:04:28.264258Z",
     "iopub.status.idle": "2025-05-16T19:04:28.297333Z",
     "shell.execute_reply": "2025-05-16T19:04:28.296125Z",
     "shell.execute_reply.started": "2025-05-16T19:04:28.265847Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16d9f3d-ad62-4c99-adb9-5c844a493e41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:04:28.299234Z",
     "iopub.status.busy": "2025-05-16T19:04:28.298509Z",
     "iopub.status.idle": "2025-05-16T19:04:28.311550Z",
     "shell.execute_reply": "2025-05-16T19:04:28.310493Z",
     "shell.execute_reply.started": "2025-05-16T19:04:28.299180Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1e0215-60a9-4cfa-9478-fbdde5a5d0b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:04:28.314850Z",
     "iopub.status.busy": "2025-05-16T19:04:28.313615Z",
     "iopub.status.idle": "2025-05-16T19:04:28.347695Z",
     "shell.execute_reply": "2025-05-16T19:04:28.346491Z",
     "shell.execute_reply.started": "2025-05-16T19:04:28.314801Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a0a88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:04:28.350832Z",
     "iopub.status.busy": "2025-05-16T19:04:28.349104Z",
     "iopub.status.idle": "2025-05-16T19:04:45.662348Z",
     "shell.execute_reply": "2025-05-16T19:04:45.661007Z",
     "shell.execute_reply.started": "2025-05-16T19:04:28.350793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff45bbfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:04:45.666004Z",
     "iopub.status.busy": "2025-05-16T19:04:45.663883Z",
     "iopub.status.idle": "2025-05-16T19:04:45.885737Z",
     "shell.execute_reply": "2025-05-16T19:04:45.884539Z",
     "shell.execute_reply.started": "2025-05-16T19:04:45.665929Z"
    }
   },
   "outputs": [],
   "source": [
    "CSV_PATH   = Path(\"logs.csv\")                         \n",
    "CSV_OUT    = CSV_PATH.with_name(CSV_PATH.stem + \"_with_labels.csv\")\n",
    "MODEL_NAME = \"byviz/bylastic_classification_logs\"\n",
    "BATCH_SIZE = 1024\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55a4aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:04:45.889762Z",
     "iopub.status.busy": "2025-05-16T19:04:45.888541Z",
     "iopub.status.idle": "2025-05-16T19:05:56.528426Z",
     "shell.execute_reply": "2025-05-16T19:05:56.526843Z",
     "shell.execute_reply.started": "2025-05-16T19:04:45.889709Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 19:05:09.375308: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-16 19:05:17.021149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b43dbcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:05:56.531976Z",
     "iopub.status.busy": "2025-05-16T19:05:56.529876Z",
     "iopub.status.idle": "2025-05-16T19:05:56.595200Z",
     "shell.execute_reply": "2025-05-16T19:05:56.593867Z",
     "shell.execute_reply.started": "2025-05-16T19:05:56.531916Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = {2: \"NORMAL\", 1: \"WARNING\", 0: \"ERROR\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "662e0738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:05:56.597409Z",
     "iopub.status.busy": "2025-05-16T19:05:56.596381Z",
     "iopub.status.idle": "2025-05-16T19:05:56.640401Z",
     "shell.execute_reply": "2025-05-16T19:05:56.639297Z",
     "shell.execute_reply.started": "2025-05-16T19:05:56.597373Z"
    }
   },
   "outputs": [],
   "source": [
    "TAG_RE   = re.compile(r\"\\[ERROR\\]\\s*:?\")   \n",
    "CLEAN_RE = re.compile(r\"^\\s+|\\s+$\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48eb5fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:05:56.643155Z",
     "iopub.status.busy": "2025-05-16T19:05:56.641811Z",
     "iopub.status.idle": "2025-05-16T19:05:56.656314Z",
     "shell.execute_reply": "2025-05-16T19:05:56.655097Z",
     "shell.execute_reply.started": "2025-05-16T19:05:56.643106Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_error_tag(line: str) -> str:\n",
    "    return CLEAN_RE.sub(\"\", TAG_RE.sub(\"\", line, count=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a612ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T19:05:56.659531Z",
     "iopub.status.busy": "2025-05-16T19:05:56.658140Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing logs:   0%|          | 0/375 [00:00<?, ?it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:04<00:00,  4.57s/it]\u001b[A\n",
      "Processing logs:   0%|          | 1/375 [00:04<28:36,  4.59s/it]A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\u001b[A\n",
      "Processing logs:   1%|          | 2/375 [00:06<18:03,  2.90s/it]A\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:03<00:03,  3.58s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]\u001b[A\n",
      "Processing logs:   1%|          | 3/375 [00:12<28:39,  4.62s/it]A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\u001b[A\n",
      "Processing logs:   1%|          | 4/375 [00:13<17:47,  2.88s/it]A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.85it/s]\u001b[A\n",
      "Processing logs:   1%|▏         | 5/375 [00:13<12:09,  1.97s/it]A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\u001b[A\n",
      "Processing logs:   2%|▏         | 6/375 [00:13<08:15,  1.34s/it]A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\u001b[A\n",
      "Processing logs:   2%|▏         | 7/375 [00:14<06:15,  1.02s/it]A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\u001b[A\n",
      "Processing logs:   2%|▏         | 8/375 [00:15<06:16,  1.02s/it]A\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:02<00:02,  2.64s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]\u001b[A\n",
      "Processing logs:   3%|▎         | 11/375 [00:18<06:42,  1.10s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:09,  4.87s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:09<00:04,  4.59s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:09<00:00,  2.73s/it]\u001b[A\n",
      "Processing logs:   3%|▎         | 12/375 [00:28<18:01,  2.98s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\u001b[A\n",
      "Processing logs:   3%|▎         | 13/375 [00:28<14:17,  2.37s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\u001b[A\n",
      "Processing logs:   4%|▎         | 14/375 [00:28<10:54,  1.81s/it]\n",
      "Batch inference:   0%|          | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:   1%|▏         | 1/79 [00:03<04:49,  3.71s/it]\u001b[A\n",
      "Batch inference:   3%|▎         | 2/79 [00:04<02:10,  1.70s/it]\u001b[A\n",
      "Batch inference:   4%|▍         | 3/79 [00:04<01:23,  1.10s/it]\u001b[A\n",
      "Batch inference:   5%|▌         | 4/79 [00:04<01:01,  1.22it/s]\u001b[A\n",
      "Batch inference:   6%|▋         | 5/79 [00:05<00:56,  1.32it/s]\u001b[A\n",
      "Batch inference:   8%|▊         | 6/79 [00:05<00:46,  1.58it/s]\u001b[A\n",
      "Batch inference:   9%|▉         | 7/79 [00:06<00:37,  1.91it/s]\u001b[A\n",
      "Batch inference:  10%|█         | 8/79 [00:09<01:33,  1.32s/it]\u001b[A\n",
      "Batch inference:  11%|█▏        | 9/79 [00:13<02:38,  2.27s/it]\u001b[A\n",
      "Batch inference:  13%|█▎        | 10/79 [00:14<02:00,  1.74s/it]\u001b[A\n",
      "Batch inference:  14%|█▍        | 11/79 [00:14<01:33,  1.38s/it]\u001b[A\n",
      "Batch inference:  15%|█▌        | 12/79 [00:15<01:17,  1.15s/it]\u001b[A\n",
      "Batch inference:  16%|█▋        | 13/79 [00:15<01:04,  1.02it/s]\u001b[A\n",
      "Batch inference:  18%|█▊        | 14/79 [00:16<00:55,  1.17it/s]\u001b[A\n",
      "Batch inference:  19%|█▉        | 15/79 [00:17<00:55,  1.15it/s]\u001b[A\n",
      "Batch inference:  20%|██        | 16/79 [00:17<00:48,  1.30it/s]\u001b[A\n",
      "Batch inference:  22%|██▏       | 17/79 [00:18<00:44,  1.40it/s]\u001b[A\n",
      "Batch inference:  23%|██▎       | 18/79 [00:19<00:49,  1.24it/s]\u001b[A\n",
      "Batch inference:  24%|██▍       | 19/79 [00:20<00:58,  1.03it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 20/79 [00:22<01:03,  1.08s/it]\u001b[A\n",
      "Batch inference:  27%|██▋       | 21/79 [00:23<01:07,  1.17s/it]\u001b[A\n",
      "Batch inference:  28%|██▊       | 22/79 [00:24<01:09,  1.22s/it]\u001b[A\n",
      "Batch inference:  29%|██▉       | 23/79 [00:26<01:10,  1.26s/it]\u001b[A\n",
      "Batch inference:  30%|███       | 24/79 [00:27<01:10,  1.29s/it]\u001b[A\n",
      "Batch inference:  32%|███▏      | 25/79 [00:28<01:10,  1.31s/it]\u001b[A\n",
      "Batch inference:  33%|███▎      | 26/79 [00:30<01:09,  1.32s/it]\u001b[A\n",
      "Batch inference:  34%|███▍      | 27/79 [00:31<01:09,  1.33s/it]\u001b[A\n",
      "Batch inference:  35%|███▌      | 28/79 [00:32<01:08,  1.34s/it]\u001b[A\n",
      "Batch inference:  37%|███▋      | 29/79 [00:34<01:07,  1.34s/it]\u001b[A\n",
      "Batch inference:  38%|███▊      | 30/79 [00:38<01:50,  2.25s/it]\u001b[A\n",
      "Batch inference:  39%|███▉      | 31/79 [00:40<01:46,  2.21s/it]\u001b[A\n",
      "Batch inference:  41%|████      | 32/79 [00:41<01:25,  1.83s/it]\u001b[A\n",
      "Batch inference:  42%|████▏     | 33/79 [00:42<01:10,  1.54s/it]\u001b[A\n",
      "Batch inference:  43%|████▎     | 34/79 [00:43<01:02,  1.40s/it]\u001b[A\n",
      "Batch inference:  44%|████▍     | 35/79 [00:45<01:05,  1.48s/it]\u001b[A\n",
      "Batch inference:  46%|████▌     | 36/79 [00:48<01:22,  1.91s/it]\u001b[A\n",
      "Batch inference:  47%|████▋     | 37/79 [00:49<01:15,  1.79s/it]\u001b[A\n",
      "Batch inference:  48%|████▊     | 38/79 [00:53<01:32,  2.24s/it]\u001b[A\n",
      "Batch inference:  49%|████▉     | 39/79 [00:57<01:55,  2.89s/it]\u001b[A\n",
      "Batch inference:  51%|█████     | 40/79 [01:01<02:10,  3.34s/it]\u001b[A\n",
      "Batch inference:  52%|█████▏    | 41/79 [01:06<02:18,  3.64s/it]\u001b[A\n",
      "Batch inference:  53%|█████▎    | 42/79 [01:10<02:22,  3.86s/it]\u001b[A\n",
      "Batch inference:  54%|█████▍    | 43/79 [01:14<02:24,  4.01s/it]\u001b[A\n",
      "Batch inference:  56%|█████▌    | 44/79 [01:19<02:24,  4.12s/it]\u001b[A\n",
      "Batch inference:  57%|█████▋    | 45/79 [01:23<02:22,  4.19s/it]\u001b[A\n",
      "Batch inference:  58%|█████▊    | 46/79 [01:28<02:20,  4.25s/it]\u001b[A\n",
      "Batch inference:  59%|█████▉    | 47/79 [01:32<02:17,  4.29s/it]\u001b[A\n",
      "Batch inference:  61%|██████    | 48/79 [01:36<02:13,  4.31s/it]\u001b[A\n",
      "Batch inference:  62%|██████▏   | 49/79 [01:41<02:09,  4.33s/it]\u001b[A\n",
      "Batch inference:  63%|██████▎   | 50/79 [01:45<02:05,  4.34s/it]\u001b[A\n",
      "Batch inference:  65%|██████▍   | 51/79 [01:48<01:51,  3.98s/it]\u001b[A\n",
      "Batch inference:  66%|██████▌   | 52/79 [01:53<01:50,  4.10s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 53/79 [01:57<01:48,  4.18s/it]\u001b[A\n",
      "Batch inference:  68%|██████▊   | 54/79 [02:01<01:41,  4.06s/it]\u001b[A\n",
      "Batch inference:  70%|██████▉   | 55/79 [02:05<01:39,  4.15s/it]\u001b[A\n",
      "Batch inference:  71%|███████   | 56/79 [02:09<01:37,  4.22s/it]\u001b[A\n",
      "Batch inference:  72%|███████▏  | 57/79 [02:14<01:33,  4.27s/it]\u001b[A\n",
      "Batch inference:  73%|███████▎  | 58/79 [02:18<01:30,  4.30s/it]\u001b[A\n",
      "Batch inference:  75%|███████▍  | 59/79 [02:23<01:26,  4.32s/it]\u001b[A\n",
      "Batch inference:  76%|███████▌  | 60/79 [02:27<01:22,  4.34s/it]\u001b[A\n",
      "Batch inference:  77%|███████▋  | 61/79 [02:31<01:18,  4.35s/it]\u001b[A\n",
      "Batch inference:  78%|███████▊  | 62/79 [02:36<01:14,  4.36s/it]\u001b[A\n",
      "Batch inference:  80%|███████▉  | 63/79 [02:40<01:09,  4.37s/it]\u001b[A\n",
      "Batch inference:  81%|████████  | 64/79 [02:45<01:07,  4.48s/it]\u001b[A\n",
      "Batch inference:  82%|████████▏ | 65/79 [02:49<01:02,  4.44s/it]\u001b[A\n",
      "Batch inference:  84%|████████▎ | 66/79 [02:54<00:57,  4.42s/it]\u001b[A\n",
      "Batch inference:  85%|████████▍ | 67/79 [02:58<00:52,  4.40s/it]\u001b[A\n",
      "Batch inference:  86%|████████▌ | 68/79 [03:02<00:48,  4.39s/it]\u001b[A\n",
      "Batch inference:  87%|████████▋ | 69/79 [03:07<00:43,  4.38s/it]\u001b[A\n",
      "Batch inference:  89%|████████▊ | 70/79 [03:11<00:39,  4.38s/it]\u001b[A\n",
      "Batch inference:  90%|████████▉ | 71/79 [03:13<00:30,  3.78s/it]\u001b[A\n",
      "Batch inference:  91%|█████████ | 72/79 [03:18<00:27,  3.96s/it]\u001b[A\n",
      "Batch inference:  92%|█████████▏| 73/79 [03:22<00:24,  4.09s/it]\u001b[A\n",
      "Batch inference:  94%|█████████▎| 74/79 [03:27<00:20,  4.18s/it]\u001b[A\n",
      "Batch inference:  95%|█████████▍| 75/79 [03:31<00:16,  4.24s/it]\u001b[A\n",
      "Batch inference:  96%|█████████▌| 76/79 [03:35<00:12,  4.28s/it]\u001b[A\n",
      "Batch inference:  97%|█████████▋| 77/79 [03:40<00:08,  4.31s/it]\u001b[A\n",
      "Batch inference:  99%|█████████▊| 78/79 [03:44<00:04,  4.33s/it]\u001b[A\n",
      "Processing logs:   4%|▍         | 15/375 [04:13<6:04:39, 60.78s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\u001b[A\n",
      "Processing logs:   4%|▍         | 16/375 [04:13<4:24:21, 44.18s/it]\n",
      "Batch inference:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  17%|█▋        | 1/6 [00:04<00:21,  4.36s/it]\u001b[A\n",
      "Batch inference:  33%|███▎      | 2/6 [00:08<00:17,  4.42s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 3/6 [00:13<00:13,  4.47s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 4/6 [00:18<00:09,  4.60s/it]\u001b[A\n",
      "Batch inference:  83%|████████▎ | 5/6 [00:23<00:04,  4.71s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 6/6 [00:27<00:00,  4.49s/it]\u001b[A\n",
      "Processing logs:   5%|▍         | 17/375 [04:41<3:55:05, 39.40s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\u001b[A\n",
      "Processing logs:   5%|▍         | 18/375 [04:41<2:48:11, 28.27s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\u001b[A\n",
      "Processing logs:   5%|▌         | 19/375 [04:42<1:59:54, 20.21s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:04<00:12,  4.12s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:08<00:08,  4.25s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:12<00:04,  4.32s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 4/4 [00:13<00:00,  2.75s/it]\u001b[A\n",
      "Processing logs:   5%|▌         | 20/375 [04:55<1:47:29, 18.17s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processing logs:   6%|▌         | 23/375 [04:55<47:20,  8.07s/it]  \n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]\u001b[A\n",
      "Processing logs:   6%|▋         | 24/375 [04:57<39:19,  6.72s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.36s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:05<00:00,  2.19s/it]\u001b[A\n",
      "Processing logs:   7%|▋         | 25/375 [05:02<36:58,  6.34s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:04<00:00,  4.01s/it]\u001b[A\n",
      "Processing logs:   7%|▋         | 26/375 [05:06<33:33,  5.77s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]\u001b[A\n",
      "Processing logs:   7%|▋         | 27/375 [05:09<29:16,  5.05s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\u001b[A\n",
      "Processing logs:   7%|▋         | 28/375 [05:09<22:13,  3.84s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\u001b[A\n",
      "Processing logs:   8%|▊         | 29/375 [05:10<16:10,  2.81s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:00<00:00,  1.71it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]\u001b[A\n",
      "Processing logs:   8%|▊         | 30/375 [05:14<18:31,  3.22s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:00<00:00,  2.10it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:02<00:00,  1.59s/it]\u001b[A\n",
      "Processing logs:   8%|▊         | 31/375 [05:17<17:54,  3.12s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\u001b[A\n",
      "Processing logs:   9%|▉         | 33/375 [05:19<12:26,  2.18s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\u001b[A\n",
      "Processing logs:   9%|▉         | 34/375 [05:19<09:44,  1.71s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\u001b[A\n",
      "Processing logs:   9%|▉         | 35/375 [05:20<08:48,  1.56s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\u001b[A\n",
      "Processing logs:  10%|▉         | 37/375 [05:21<05:52,  1.04s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:08,  4.39s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:05<00:02,  2.33s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:05<00:00,  1.32s/it]\u001b[A\n",
      "Processing logs:  10%|█         | 38/375 [05:26<11:35,  2.06s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:03<00:09,  3.20s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:07<00:07,  3.92s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:12<00:04,  4.14s/it]\u001b[A\n",
      "Processing logs:  10%|█         | 39/375 [05:38<25:36,  4.57s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:07<00:07,  3.80s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:11<00:04,  4.08s/it]\u001b[A\n",
      "Processing logs:  11%|█         | 40/375 [05:50<36:10,  6.48s/it]\n",
      "Batch inference:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  17%|█▋        | 1/6 [00:03<00:15,  3.12s/it]\u001b[A\n",
      "Batch inference:  33%|███▎      | 2/6 [00:06<00:12,  3.15s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 3/6 [00:09<00:09,  3.12s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 4/6 [00:12<00:06,  3.10s/it]\u001b[A\n",
      "Batch inference:  83%|████████▎ | 5/6 [00:16<00:03,  3.56s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 6/6 [00:20<00:00,  3.75s/it]\u001b[A\n",
      "Processing logs:  11%|█         | 41/375 [06:11<58:07, 10.44s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\u001b[A\n",
      "Processing logs:  11%|█         | 42/375 [06:11<42:02,  7.57s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.36s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:04<00:00,  1.91s/it]\u001b[A\n",
      "Processing logs:  11%|█▏        | 43/375 [06:16<37:09,  6.72s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\u001b[A\n",
      "Processing logs:  12%|█▏        | 44/375 [06:18<29:01,  5.26s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.53it/s]\u001b[A\n",
      "Processing logs:  12%|█▏        | 45/375 [06:18<20:43,  3.77s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\u001b[A\n",
      "Processing logs:  12%|█▏        | 46/375 [06:18<14:53,  2.72s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:01<00:01,  1.74s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\u001b[A\n",
      "Processing logs:  13%|█▎        | 48/375 [06:22<12:17,  2.26s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\u001b[A\n",
      "Processing logs:  13%|█▎        | 49/375 [06:22<09:48,  1.80s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\u001b[A\n",
      "Processing logs:  13%|█▎        | 50/375 [06:23<08:05,  1.49s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:03<00:03,  3.87s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\u001b[A\n",
      "Processing logs:  14%|█▍        | 54/375 [06:27<06:25,  1.20s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\u001b[A\n",
      "Processing logs:  15%|█▍        | 55/375 [06:27<05:31,  1.04s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\u001b[A\n",
      "Processing logs:  15%|█▍        | 56/375 [06:27<04:43,  1.12it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\u001b[A\n",
      "Processing logs:  15%|█▌        | 57/375 [06:28<04:34,  1.16it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\u001b[A\n",
      "Processing logs:  15%|█▌        | 58/375 [06:30<05:56,  1.12s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\u001b[A\n",
      "Processing logs:  16%|█▌        | 59/375 [06:31<05:42,  1.08s/it]\n",
      "Batch inference:   0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:   6%|▌         | 1/17 [00:04<01:09,  4.37s/it]\u001b[A\n",
      "Batch inference:  12%|█▏        | 2/17 [00:05<00:40,  2.68s/it]\u001b[A\n",
      "Batch inference:  18%|█▊        | 3/17 [00:10<00:48,  3.45s/it]\u001b[A\n",
      "Batch inference:  24%|██▎       | 4/17 [00:14<00:49,  3.83s/it]\u001b[A\n",
      "Batch inference:  29%|██▉       | 5/17 [00:16<00:35,  2.99s/it]\u001b[A\n",
      "Batch inference:  35%|███▌      | 6/17 [00:20<00:38,  3.46s/it]\u001b[A\n",
      "Batch inference:  41%|████      | 7/17 [00:24<00:37,  3.77s/it]\u001b[A\n",
      "Batch inference:  47%|████▋     | 8/17 [00:27<00:30,  3.39s/it]\u001b[A\n",
      "Batch inference:  53%|█████▎    | 9/17 [00:31<00:29,  3.70s/it]\u001b[A\n",
      "Batch inference:  59%|█████▉    | 10/17 [00:36<00:28,  4.03s/it]\u001b[A\n",
      "Batch inference:  65%|██████▍   | 11/17 [00:38<00:19,  3.33s/it]\u001b[A\n",
      "Batch inference:  71%|███████   | 12/17 [00:38<00:12,  2.41s/it]\u001b[A\n",
      "Batch inference:  76%|███████▋  | 13/17 [00:41<00:10,  2.59s/it]\u001b[A\n",
      "Batch inference:  82%|████████▏ | 14/17 [00:42<00:05,  1.95s/it]\u001b[A\n",
      "Batch inference:  88%|████████▊ | 15/17 [00:42<00:02,  1.49s/it]\u001b[A\n",
      "Batch inference:  94%|█████████▍| 16/17 [00:43<00:01,  1.22s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 17/17 [00:43<00:00,  1.09it/s]\u001b[A\n",
      "Processing logs:  16%|█▋        | 61/375 [07:14<51:22,  9.82s/it][A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\u001b[A\n",
      "Processing logs:  17%|█▋        | 62/375 [07:15<40:14,  7.71s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\u001b[A\n",
      "Processing logs:  17%|█▋        | 63/375 [07:16<31:40,  6.09s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\u001b[A\n",
      "Processing logs:  17%|█▋        | 64/375 [07:17<23:40,  4.57s/it]\n",
      "Batch inference:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  14%|█▍        | 1/7 [00:01<00:07,  1.19s/it]\u001b[A\n",
      "Batch inference:  29%|██▊       | 2/7 [00:01<00:03,  1.39it/s]\u001b[A\n",
      "Batch inference:  43%|████▎     | 3/7 [00:01<00:02,  1.76it/s]\u001b[A\n",
      "Batch inference:  57%|█████▋    | 4/7 [00:06<00:06,  2.03s/it]\u001b[A\n",
      "Batch inference:  71%|███████▏  | 5/7 [00:10<00:05,  2.88s/it]\u001b[A\n",
      "Batch inference:  86%|████████▌ | 6/7 [00:15<00:03,  3.39s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 7/7 [00:15<00:00,  2.39s/it]\u001b[A\n",
      "Processing logs:  17%|█▋        | 65/375 [07:32<38:47,  7.51s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processing logs:  18%|█▊        | 68/375 [07:32<17:49,  3.48s/it]\n",
      "Batch inference:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  12%|█▎        | 1/8 [00:04<00:29,  4.29s/it]\u001b[A\n",
      "Batch inference:  25%|██▌       | 2/8 [00:08<00:25,  4.31s/it]\u001b[A\n",
      "Batch inference:  38%|███▊      | 3/8 [00:12<00:21,  4.32s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 4/8 [00:16<00:16,  4.17s/it]\u001b[A\n",
      "Batch inference:  62%|██████▎   | 5/8 [00:17<00:08,  2.82s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 6/8 [00:21<00:06,  3.35s/it]\u001b[A\n",
      "Batch inference:  88%|████████▊ | 7/8 [00:26<00:03,  3.70s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 8/8 [00:27<00:00,  3.00s/it]\u001b[A\n",
      "Processing logs:  18%|█▊        | 69/375 [08:00<42:54,  8.41s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.60it/s]\u001b[A\n",
      "Processing logs:  19%|█▊        | 70/375 [08:00<33:17,  6.55s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\u001b[A\n",
      "Processing logs:  19%|█▉        | 71/375 [08:00<25:47,  5.09s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\u001b[A\n",
      "Processing logs:  19%|█▉        | 72/375 [08:01<19:47,  3.92s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:01<00:01,  1.63s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]\u001b[A\n",
      "Processing logs:  19%|█▉        | 73/375 [08:06<21:41,  4.31s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\u001b[A\n",
      "Processing logs:  20%|█▉        | 74/375 [08:07<16:36,  3.31s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\u001b[A\n",
      "Processing logs:  20%|██        | 75/375 [08:08<13:25,  2.68s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\u001b[A\n",
      "Processing logs:  20%|██        | 76/375 [08:10<11:47,  2.37s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:04<00:13,  4.34s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:08<00:08,  4.36s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:13<00:04,  4.37s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 4/4 [00:13<00:00,  2.70s/it]\u001b[A\n",
      "Processing logs:  21%|██        | 77/375 [08:23<27:33,  5.55s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:00<00:00,  1.29it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\u001b[A\n",
      "Processing logs:  21%|██        | 78/375 [08:24<21:05,  4.26s/it]\n",
      "Batch inference:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  11%|█         | 1/9 [00:00<00:03,  2.06it/s]\u001b[A\n",
      "Batch inference:  22%|██▏       | 2/9 [00:00<00:03,  2.19it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 3/9 [00:04<00:10,  1.81s/it]\u001b[A\n",
      "Batch inference:  44%|████▍     | 4/9 [00:08<00:14,  2.83s/it]\u001b[A\n",
      "Batch inference:  56%|█████▌    | 5/9 [00:13<00:13,  3.39s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 6/9 [00:13<00:07,  2.46s/it]\u001b[A\n",
      "Batch inference:  78%|███████▊  | 7/9 [00:14<00:03,  1.86s/it]\u001b[A\n",
      "Batch inference:  89%|████████▉ | 8/9 [00:15<00:01,  1.47s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 9/9 [00:17<00:00,  1.65s/it]\u001b[A\n",
      "Processing logs:  22%|██▏       | 81/375 [08:41<24:49,  5.07s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\u001b[A\n",
      "Processing logs:  22%|██▏       | 82/375 [08:42<19:59,  4.09s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\u001b[A\n",
      "Processing logs:  22%|██▏       | 84/375 [08:43<13:13,  2.73s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\u001b[A\n",
      "Processing logs:  23%|██▎       | 85/375 [08:43<11:09,  2.31s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:03<00:06,  3.19s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:07<00:03,  3.89s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:07<00:00,  2.22s/it]\u001b[A\n",
      "Processing logs:  23%|██▎       | 86/375 [08:51<17:16,  3.59s/it]\n",
      "Batch inference:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  20%|██        | 1/5 [00:04<00:17,  4.36s/it]\u001b[A\n",
      "Batch inference:  40%|████      | 2/5 [00:08<00:13,  4.38s/it]\u001b[A\n",
      "Batch inference:  60%|██████    | 3/5 [00:13<00:09,  4.53s/it]\u001b[A\n",
      "Batch inference:  80%|████████  | 4/5 [00:17<00:04,  4.48s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 5/5 [00:20<00:00,  3.65s/it]\u001b[A\n",
      "Processing logs:  23%|██▎       | 87/375 [09:11<36:53,  7.69s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\u001b[A\n",
      "Processing logs:  23%|██▎       | 88/375 [09:11<27:25,  5.73s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\u001b[A\n",
      "Processing logs:  24%|██▍       | 90/375 [09:12<15:58,  3.36s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\u001b[A\n",
      "Processing logs:  24%|██▍       | 91/375 [09:13<13:18,  2.81s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\u001b[A\n",
      "Processing logs:  25%|██▍       | 92/375 [09:13<10:06,  2.14s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\u001b[A\n",
      "Processing logs:  25%|██▍       | 93/375 [09:13<07:37,  1.62s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:08,  4.38s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:08<00:04,  4.40s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:12<00:00,  4.17s/it]\u001b[A\n",
      "Processing logs:  25%|██▌       | 94/375 [09:26<21:50,  4.66s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\u001b[A\n",
      "Processing logs:  25%|██▌       | 95/375 [09:30<20:23,  4.37s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\u001b[A\n",
      "Processing logs:  26%|██▌       | 96/375 [09:30<14:41,  3.16s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.35s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:04<00:00,  1.90s/it]\u001b[A\n",
      "Processing logs:  26%|██▌       | 97/375 [09:34<16:30,  3.56s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\u001b[A\n",
      "Processing logs:  26%|██▌       | 98/375 [09:36<14:26,  3.13s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\u001b[A\n",
      "Processing logs:  26%|██▋       | 99/375 [09:39<13:14,  2.88s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\u001b[A\n",
      "Processing logs:  27%|██▋       | 100/375 [09:41<12:58,  2.83s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.42s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:05<00:00,  2.21s/it]\u001b[A\n",
      "Processing logs:  27%|██▋       | 101/375 [09:46<16:00,  3.50s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\u001b[A\n",
      "Processing logs:  27%|██▋       | 102/375 [09:47<12:00,  2.64s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\u001b[A\n",
      "Processing logs:  27%|██▋       | 103/375 [09:49<11:39,  2.57s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:01<00:03,  1.98s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:02<00:00,  1.00it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:02<00:00,  1.38it/s]\u001b[A\n",
      "Processing logs:  28%|██▊       | 104/375 [09:52<11:48,  2.61s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\u001b[A\n",
      "Processing logs:  28%|██▊       | 105/375 [09:54<10:13,  2.27s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:03<00:03,  3.23s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]\u001b[A\n",
      "Processing logs:  28%|██▊       | 106/375 [09:57<11:40,  2.60s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\u001b[A\n",
      "Processing logs:  29%|██▊       | 107/375 [09:57<08:33,  1.91s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\u001b[A\n",
      "Processing logs:  29%|██▉       | 108/375 [09:58<07:03,  1.59s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\u001b[A\n",
      "Processing logs:  29%|██▉       | 109/375 [09:59<06:06,  1.38s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\u001b[A\n",
      "Processing logs:  29%|██▉       | 110/375 [09:59<04:51,  1.10s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\u001b[A\n",
      "Processing logs:  30%|██▉       | 111/375 [10:00<03:56,  1.12it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\u001b[A\n",
      "Processing logs:  30%|██▉       | 112/375 [10:01<04:03,  1.08it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\u001b[A\n",
      "Processing logs:  30%|███       | 113/375 [10:01<03:05,  1.41it/s]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:08,  4.37s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:08<00:04,  4.38s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:10<00:00,  3.20s/it]\u001b[A\n",
      "Processing logs:  30%|███       | 114/375 [10:12<15:57,  3.67s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.45it/s]\u001b[A\n",
      "Processing logs:  31%|███       | 115/375 [10:12<11:23,  2.63s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\u001b[A\n",
      "Processing logs:  31%|███       | 116/375 [10:12<08:14,  1.91s/it]\n",
      "Batch inference:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  20%|██        | 1/5 [00:01<00:06,  1.71s/it]\u001b[A\n",
      "Batch inference:  40%|████      | 2/5 [00:02<00:04,  1.40s/it]\u001b[A\n",
      "Batch inference:  60%|██████    | 3/5 [00:03<00:02,  1.21s/it]\u001b[A\n",
      "Batch inference:  80%|████████  | 4/5 [00:04<00:01,  1.17s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 5/5 [00:07<00:00,  1.55s/it]\u001b[A\n",
      "Processing logs:  31%|███       | 117/375 [10:19<15:04,  3.50s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\u001b[A\n",
      "Processing logs:  31%|███▏      | 118/375 [10:19<10:40,  2.49s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\u001b[A\n",
      "Processing logs:  32%|███▏      | 119/375 [10:21<09:05,  2.13s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.72it/s]\u001b[A\n",
      "Processing logs:  32%|███▏      | 120/375 [10:21<06:32,  1.54s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\u001b[A\n",
      "Processing logs:  32%|███▏      | 121/375 [10:23<07:53,  1.86s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\u001b[A\n",
      "Processing logs:  33%|███▎      | 122/375 [10:27<09:29,  2.25s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.38s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]\u001b[A\n",
      "Processing logs:  33%|███▎      | 123/375 [10:32<12:53,  3.07s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:02<00:05,  2.91s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:05<00:02,  2.92s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:06<00:00,  1.92s/it]\u001b[A\n",
      "Processing logs:  33%|███▎      | 124/375 [10:38<17:15,  4.12s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\u001b[A\n",
      "Processing logs:  33%|███▎      | 125/375 [10:41<15:18,  3.67s/it]\n",
      "Batch inference:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  14%|█▍        | 1/7 [00:04<00:26,  4.37s/it]\u001b[A\n",
      "Batch inference:  29%|██▊       | 2/7 [00:05<00:10,  2.19s/it]\u001b[A\n",
      "Batch inference:  43%|████▎     | 3/7 [00:06<00:06,  1.72s/it]\u001b[A\n",
      "Batch inference:  57%|█████▋    | 4/7 [00:06<00:03,  1.29s/it]\u001b[A\n",
      "Batch inference:  71%|███████▏  | 5/7 [00:08<00:02,  1.27s/it]\u001b[A\n",
      "Batch inference:  86%|████████▌ | 6/7 [00:08<00:01,  1.16s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 7/7 [00:10<00:00,  1.20s/it]\u001b[A\n",
      "Processing logs:  34%|███▎      | 126/375 [10:51<23:30,  5.66s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:04<00:13,  4.36s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:07<00:07,  3.51s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:10<00:03,  3.19s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 4/4 [00:11<00:00,  2.42s/it]\u001b[A\n",
      "Processing logs:  34%|███▍      | 127/375 [11:02<30:26,  7.36s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\u001b[A\n",
      "Processing logs:  34%|███▍      | 128/375 [11:04<22:29,  5.47s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\u001b[A\n",
      "Processing logs:  34%|███▍      | 129/375 [11:04<15:58,  3.89s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:03<00:03,  3.27s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]\u001b[A\n",
      "Processing logs:  35%|███▍      | 130/375 [11:10<18:37,  4.56s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\u001b[A\n",
      "Processing logs:  35%|███▍      | 131/375 [11:11<14:25,  3.55s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\u001b[A\n",
      "Processing logs:  35%|███▌      | 132/375 [11:11<10:29,  2.59s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\u001b[A\n",
      "Processing logs:  35%|███▌      | 133/375 [11:12<07:35,  1.88s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\u001b[A\n",
      "Processing logs:  36%|███▌      | 134/375 [11:12<05:54,  1.47s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it]\u001b[A\n",
      "Processing logs:  36%|███▌      | 135/375 [11:15<07:09,  1.79s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\u001b[A\n",
      "Processing logs:  37%|███▋      | 137/375 [11:15<04:09,  1.05s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\u001b[A\n",
      "Processing logs:  37%|███▋      | 138/375 [11:15<03:18,  1.19it/s]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:01<00:03,  1.66s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:04<00:02,  2.21s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:04<00:00,  1.26s/it]\u001b[A\n",
      "Processing logs:  37%|███▋      | 139/375 [11:20<06:56,  1.77s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\u001b[A\n",
      "Processing logs:  37%|███▋      | 140/375 [11:20<05:42,  1.46s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\u001b[A\n",
      "Processing logs:  38%|███▊      | 141/375 [11:21<04:45,  1.22s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\u001b[A\n",
      "Processing logs:  38%|███▊      | 142/375 [11:22<05:02,  1.30s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:01<00:01,  1.99s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]\u001b[A\n",
      "Processing logs:  38%|███▊      | 143/375 [11:25<06:05,  1.58s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\u001b[A\n",
      "Processing logs:  38%|███▊      | 144/375 [11:25<04:32,  1.18s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\u001b[A\n",
      "Processing logs:  39%|███▊      | 145/375 [11:25<03:26,  1.11it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\u001b[A\n",
      "Processing logs:  39%|███▉      | 146/375 [11:25<02:34,  1.48it/s]\n",
      "Batch inference:   0%|          | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:   3%|▎         | 1/29 [00:04<02:02,  4.38s/it]\u001b[A\n",
      "Batch inference:   7%|▋         | 2/29 [00:08<01:58,  4.40s/it]\u001b[A\n",
      "Batch inference:  10%|█         | 3/29 [00:13<01:54,  4.40s/it]\u001b[A\n",
      "Batch inference:  14%|█▍        | 4/29 [00:17<01:50,  4.41s/it]\u001b[A\n",
      "Batch inference:  17%|█▋        | 5/29 [00:22<01:45,  4.41s/it]\u001b[A\n",
      "Batch inference:  21%|██        | 6/29 [00:26<01:41,  4.40s/it]\u001b[A\n",
      "Batch inference:  24%|██▍       | 7/29 [00:30<01:36,  4.39s/it]\u001b[A\n",
      "Batch inference:  28%|██▊       | 8/29 [00:35<01:32,  4.39s/it]\u001b[A\n",
      "Batch inference:  31%|███       | 9/29 [00:39<01:27,  4.40s/it]\u001b[A\n",
      "Batch inference:  34%|███▍      | 10/29 [00:44<01:23,  4.41s/it]\u001b[A\n",
      "Batch inference:  38%|███▊      | 11/29 [00:48<01:19,  4.40s/it]\u001b[A\n",
      "Batch inference:  41%|████▏     | 12/29 [00:52<01:14,  4.40s/it]\u001b[A\n",
      "Batch inference:  45%|████▍     | 13/29 [00:57<01:10,  4.40s/it]\u001b[A\n",
      "Batch inference:  48%|████▊     | 14/29 [01:01<01:06,  4.41s/it]\u001b[A\n",
      "Batch inference:  52%|█████▏    | 15/29 [01:06<01:01,  4.41s/it]\u001b[A\n",
      "Batch inference:  55%|█████▌    | 16/29 [01:10<00:57,  4.41s/it]\u001b[A\n",
      "Batch inference:  59%|█████▊    | 17/29 [01:14<00:52,  4.41s/it]\u001b[A\n",
      "Batch inference:  62%|██████▏   | 18/29 [01:15<00:35,  3.22s/it]\u001b[A\n",
      "Batch inference:  66%|██████▌   | 19/29 [01:16<00:26,  2.65s/it]\u001b[A\n",
      "Batch inference:  69%|██████▉   | 20/29 [01:21<00:28,  3.17s/it]\u001b[A\n",
      "Batch inference:  72%|███████▏  | 21/29 [01:25<00:28,  3.55s/it]\u001b[A\n",
      "Batch inference:  76%|███████▌  | 22/29 [01:29<00:26,  3.81s/it]\u001b[A\n",
      "Batch inference:  79%|███████▉  | 23/29 [01:32<00:21,  3.58s/it]\u001b[A\n",
      "Batch inference:  83%|████████▎ | 24/29 [01:37<00:19,  3.82s/it]\u001b[A\n",
      "Batch inference:  86%|████████▌ | 25/29 [01:41<00:15,  4.00s/it]\u001b[A\n",
      "Batch inference:  90%|████████▉ | 26/29 [01:46<00:12,  4.11s/it]\u001b[A\n",
      "Batch inference:  93%|█████████▎| 27/29 [01:47<00:06,  3.23s/it]\u001b[A\n",
      "Batch inference:  97%|█████████▋| 28/29 [01:48<00:02,  2.61s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 29/29 [01:50<00:00,  2.32s/it]\u001b[A\n",
      "Processing logs:  39%|███▉      | 147/375 [13:15<2:06:16, 33.23s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\u001b[A\n",
      "Processing logs:  39%|███▉      | 148/375 [13:16<1:28:57, 23.51s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\u001b[A\n",
      "Processing logs:  40%|███▉      | 149/375 [13:17<1:02:43, 16.65s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\u001b[A\n",
      "Processing logs:  40%|████      | 150/375 [13:17<44:29, 11.87s/it]  \n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:02<00:04,  2.45s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:04<00:02,  2.47s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:05<00:00,  1.61s/it]\u001b[A\n",
      "Processing logs:  40%|████      | 151/375 [13:23<37:14,  9.97s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.26it/s]\u001b[A\n",
      "Processing logs:  41%|████      | 152/375 [13:23<26:09,  7.04s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\u001b[A\n",
      "Processing logs:  41%|████      | 153/375 [13:23<18:50,  5.09s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\u001b[A\n",
      "Processing logs:  41%|████      | 154/375 [13:24<14:07,  3.83s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\u001b[A\n",
      "Processing logs:  41%|████▏     | 155/375 [13:25<11:04,  3.02s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:08,  4.37s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:07<00:03,  3.72s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:10<00:00,  3.43s/it]\u001b[A\n",
      "Processing logs:  42%|████▏     | 156/375 [13:36<19:29,  5.34s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\u001b[A\n",
      "Processing logs:  42%|████▏     | 158/375 [13:37<11:02,  3.05s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:08,  4.38s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:06<00:03,  3.33s/it]\u001b[A\n",
      "Processing logs:  42%|████▏     | 159/375 [13:44<14:32,  4.04s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\u001b[A\n",
      "Processing logs:  43%|████▎     | 160/375 [13:45<11:21,  3.17s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\u001b[A\n",
      "Processing logs:  43%|████▎     | 161/375 [13:46<09:42,  2.72s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:08,  4.39s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:08<00:04,  4.39s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:10<00:00,  3.20s/it]\u001b[A\n",
      "Processing logs:  43%|████▎     | 162/375 [13:57<17:28,  4.92s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\u001b[A\n",
      "Processing logs:  44%|████▎     | 164/375 [13:57<10:00,  2.85s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\u001b[A\n",
      "Processing logs:  44%|████▍     | 165/375 [13:59<08:45,  2.50s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\u001b[A\n",
      "Processing logs:  44%|████▍     | 166/375 [14:00<07:27,  2.14s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.26s/it]\u001b[A\n",
      "Processing logs:  45%|████▍     | 167/375 [14:04<09:26,  2.72s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\u001b[A\n",
      "Processing logs:  45%|████▍     | 168/375 [14:06<09:00,  2.61s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\u001b[A\n",
      "Processing logs:  45%|████▌     | 169/375 [14:07<06:38,  1.93s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\u001b[A\n",
      "Processing logs:  45%|████▌     | 170/375 [14:07<04:53,  1.43s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\u001b[A\n",
      "Processing logs:  46%|████▌     | 171/375 [14:07<03:40,  1.08s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\u001b[A\n",
      "Processing logs:  46%|████▌     | 172/375 [14:07<02:57,  1.14it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.25s/it]\u001b[A\n",
      "Processing logs:  46%|████▌     | 173/375 [14:10<04:20,  1.29s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processing logs:  49%|████▉     | 183/375 [14:10<00:48,  3.92it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\u001b[A\n",
      "                                                              \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\u001b[A\n",
      "                                                              \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\u001b[A\n",
      "                                                              \u001b[A\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:02<00:04,  2.09s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:06<00:03,  3.64s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:06<00:00,  2.03s/it]\u001b[A\n",
      "Processing logs:  50%|████▉     | 187/375 [14:18<02:31,  1.24it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\u001b[A\n",
      "                                                              \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n",
      "Processing logs:  51%|█████     | 190/375 [14:21<02:42,  1.14it/s]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:03<00:11,  3.76s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:04<00:03,  1.78s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 4/4 [00:05<00:00,  1.07s/it]\u001b[A\n",
      "                                                              \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processing logs:  51%|█████     | 192/375 [14:27<03:55,  1.29s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\u001b[A\n",
      "                                                              \u001b[A\n",
      "Batch inference:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  20%|██        | 1/5 [00:03<00:13,  3.32s/it]\u001b[A\n",
      "Processing logs:  51%|█████     | 192/375 [14:38<03:55,  1.29s/it]\n",
      "Batch inference:  60%|██████    | 3/5 [00:12<00:08,  4.16s/it]\u001b[A\n",
      "Batch inference:  80%|████████  | 4/5 [00:16<00:04,  4.33s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 5/5 [00:18<00:00,  3.49s/it]\u001b[A\n",
      "Processing logs:  52%|█████▏    | 194/375 [14:47<09:37,  3.19s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\u001b[A\n",
      "Processing logs:  52%|█████▏    | 195/375 [14:47<08:20,  2.78s/it]\n",
      "Batch inference:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  20%|██        | 1/5 [00:00<00:02,  1.43it/s]\u001b[A\n",
      "Batch inference:  40%|████      | 2/5 [00:00<00:01,  2.48it/s]\u001b[A\n",
      "Batch inference:  60%|██████    | 3/5 [00:01<00:00,  2.98it/s]\u001b[A\n",
      "Batch inference:  80%|████████  | 4/5 [00:01<00:00,  3.55it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 5/5 [00:01<00:00,  4.16it/s]\u001b[A\n",
      "Processing logs:  53%|█████▎    | 197/375 [14:49<06:35,  2.22s/it]\n",
      "Batch inference:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:   8%|▊         | 1/13 [00:02<00:24,  2.04s/it]\u001b[A\n",
      "Batch inference:  15%|█▌        | 2/13 [00:04<00:22,  2.05s/it]\u001b[A\n",
      "Batch inference:  23%|██▎       | 3/13 [00:06<00:21,  2.17s/it]\u001b[A\n",
      "Batch inference:  31%|███       | 4/13 [00:10<00:27,  3.06s/it]\u001b[A\n",
      "Batch inference:  38%|███▊      | 5/13 [00:15<00:28,  3.55s/it]\u001b[A\n",
      "Batch inference:  46%|████▌     | 6/13 [00:19<00:26,  3.85s/it]\u001b[A\n",
      "Batch inference:  54%|█████▍    | 7/13 [00:21<00:19,  3.26s/it]\u001b[A\n",
      "Batch inference:  62%|██████▏   | 8/13 [00:23<00:14,  2.88s/it]\u001b[A\n",
      "Batch inference:  69%|██████▉   | 9/13 [00:25<00:10,  2.62s/it]\u001b[A\n",
      "Batch inference:  77%|███████▋  | 10/13 [00:27<00:07,  2.40s/it]\u001b[A\n",
      "Batch inference:  85%|████████▍ | 11/13 [00:29<00:04,  2.30s/it]\u001b[A\n",
      "Batch inference:  92%|█████████▏| 12/13 [00:30<00:01,  1.80s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 13/13 [00:33<00:00,  2.06s/it]\u001b[A\n",
      "Processing logs:  53%|█████▎    | 198/375 [15:22<21:38,  7.34s/it]A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\u001b[A\n",
      "Processing logs:  53%|█████▎    | 199/375 [15:23<17:47,  6.07s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\u001b[A\n",
      "Processing logs:  53%|█████▎    | 200/375 [15:26<15:44,  5.40s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\u001b[A\n",
      "Processing logs:  54%|█████▎    | 201/375 [15:28<13:50,  4.77s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\u001b[A\n",
      "Processing logs:  54%|█████▍    | 202/375 [15:30<11:28,  3.98s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\u001b[A\n",
      "Processing logs:  54%|█████▍    | 203/375 [15:33<10:17,  3.59s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:   1%|          | 1/95 [00:01<02:54,  1.86s/it]\u001b[A\n",
      "Batch inference:   2%|▏         | 2/95 [00:02<01:54,  1.23s/it]\u001b[A\n",
      "Batch inference:   3%|▎         | 3/95 [00:03<01:33,  1.02s/it]\u001b[A\n",
      "Batch inference:   4%|▍         | 4/95 [00:06<03:00,  1.99s/it]\u001b[A\n",
      "Batch inference:   5%|▌         | 5/95 [00:11<04:17,  2.86s/it]\u001b[A\n",
      "Batch inference:   6%|▋         | 6/95 [00:15<05:00,  3.38s/it]\u001b[A\n",
      "Batch inference:   7%|▋         | 7/95 [00:20<05:26,  3.71s/it]\u001b[A\n",
      "Batch inference:   8%|▊         | 8/95 [00:24<05:41,  3.93s/it]\u001b[A\n",
      "Batch inference:   9%|▉         | 9/95 [00:28<05:49,  4.07s/it]\u001b[A\n",
      "Batch inference:  11%|█         | 10/95 [00:33<05:54,  4.17s/it]\u001b[A\n",
      "Batch inference:  12%|█▏        | 11/95 [00:37<05:56,  4.24s/it]\u001b[A\n",
      "Batch inference:  13%|█▎        | 12/95 [00:42<06:04,  4.39s/it]\u001b[A\n",
      "Batch inference:  14%|█▎        | 13/95 [00:46<05:59,  4.39s/it]\u001b[A\n",
      "Batch inference:  15%|█▍        | 14/95 [00:51<05:55,  4.39s/it]\u001b[A\n",
      "Batch inference:  16%|█▌        | 15/95 [00:55<05:50,  4.39s/it]\u001b[A\n",
      "Batch inference:  17%|█▋        | 16/95 [00:59<05:46,  4.39s/it]\u001b[A\n",
      "Batch inference:  18%|█▊        | 17/95 [01:04<05:42,  4.39s/it]\u001b[A\n",
      "Batch inference:  19%|█▉        | 18/95 [01:08<05:38,  4.40s/it]\u001b[A\n",
      "Batch inference:  20%|██        | 19/95 [01:13<05:34,  4.40s/it]\u001b[A\n",
      "Batch inference:  21%|██        | 20/95 [01:17<05:29,  4.39s/it]\u001b[A\n",
      "Batch inference:  22%|██▏       | 21/95 [01:21<05:24,  4.39s/it]\u001b[A\n",
      "Batch inference:  23%|██▎       | 22/95 [01:26<05:20,  4.39s/it]\u001b[A\n",
      "Batch inference:  24%|██▍       | 23/95 [01:30<05:16,  4.39s/it]\u001b[A\n",
      "Batch inference:  25%|██▌       | 24/95 [01:35<05:11,  4.39s/it]\u001b[A\n",
      "Batch inference:  26%|██▋       | 25/95 [01:39<05:07,  4.39s/it]\u001b[A\n",
      "Batch inference:  27%|██▋       | 26/95 [01:43<05:02,  4.39s/it]\u001b[A\n",
      "Batch inference:  28%|██▊       | 27/95 [01:48<04:58,  4.39s/it]\u001b[A\n",
      "Batch inference:  29%|██▉       | 28/95 [01:52<04:53,  4.39s/it]\u001b[A\n",
      "Batch inference:  31%|███       | 29/95 [01:57<04:49,  4.39s/it]\u001b[A\n",
      "Batch inference:  32%|███▏      | 30/95 [02:01<04:45,  4.40s/it]\u001b[A\n",
      "Batch inference:  33%|███▎      | 31/95 [02:05<04:41,  4.40s/it]\u001b[A\n",
      "Batch inference:  34%|███▎      | 32/95 [02:10<04:36,  4.40s/it]\u001b[A\n",
      "Batch inference:  35%|███▍      | 33/95 [02:14<04:32,  4.39s/it]\u001b[A\n",
      "Batch inference:  36%|███▌      | 34/95 [02:18<04:27,  4.39s/it]\u001b[A\n",
      "Batch inference:  37%|███▋      | 35/95 [02:23<04:23,  4.39s/it]\u001b[A\n",
      "Batch inference:  38%|███▊      | 36/95 [02:27<04:19,  4.39s/it]\u001b[A\n",
      "Batch inference:  39%|███▉      | 37/95 [02:32<04:15,  4.40s/it]\u001b[A\n",
      "Batch inference:  40%|████      | 38/95 [02:36<04:10,  4.39s/it]\u001b[A\n",
      "Batch inference:  41%|████      | 39/95 [02:40<04:05,  4.39s/it]\u001b[A\n",
      "Batch inference:  42%|████▏     | 40/95 [02:45<04:01,  4.39s/it]\u001b[A\n",
      "Batch inference:  43%|████▎     | 41/95 [02:49<03:57,  4.40s/it]\u001b[A\n",
      "Batch inference:  44%|████▍     | 42/95 [02:54<03:53,  4.40s/it]\u001b[A\n",
      "Batch inference:  45%|████▌     | 43/95 [02:58<03:48,  4.40s/it]\u001b[A\n",
      "Batch inference:  46%|████▋     | 44/95 [03:02<03:44,  4.39s/it]\u001b[A\n",
      "Batch inference:  47%|████▋     | 45/95 [03:07<03:39,  4.39s/it]\u001b[A\n",
      "Batch inference:  48%|████▊     | 46/95 [03:11<03:35,  4.39s/it]\u001b[A\n",
      "Batch inference:  49%|████▉     | 47/95 [03:16<03:30,  4.39s/it]\u001b[A\n",
      "Batch inference:  51%|█████     | 48/95 [03:20<03:26,  4.39s/it]\u001b[A\n",
      "Batch inference:  52%|█████▏    | 49/95 [03:24<03:22,  4.40s/it]\u001b[A\n",
      "Batch inference:  53%|█████▎    | 50/95 [03:29<03:17,  4.39s/it]\u001b[A\n",
      "Batch inference:  54%|█████▎    | 51/95 [03:33<03:13,  4.39s/it]\u001b[A\n",
      "Batch inference:  55%|█████▍    | 52/95 [03:38<03:08,  4.39s/it]\u001b[A\n",
      "Batch inference:  56%|█████▌    | 53/95 [03:42<03:04,  4.39s/it]\u001b[A\n",
      "Batch inference:  57%|█████▋    | 54/95 [03:46<02:59,  4.39s/it]\u001b[A\n",
      "Batch inference:  58%|█████▊    | 55/95 [03:51<02:55,  4.39s/it]\u001b[A\n",
      "Batch inference:  59%|█████▉    | 56/95 [03:55<02:51,  4.39s/it]\u001b[A\n",
      "Batch inference:  60%|██████    | 57/95 [04:00<02:46,  4.39s/it]\u001b[A\n",
      "Batch inference:  61%|██████    | 58/95 [04:04<02:42,  4.39s/it]\u001b[A\n",
      "Batch inference:  62%|██████▏   | 59/95 [04:08<02:37,  4.39s/it]\u001b[A\n",
      "Batch inference:  63%|██████▎   | 60/95 [04:13<02:33,  4.39s/it]\u001b[A\n",
      "Batch inference:  64%|██████▍   | 61/95 [04:17<02:29,  4.39s/it]\u001b[A\n",
      "Batch inference:  65%|██████▌   | 62/95 [04:21<02:24,  4.38s/it]\u001b[A\n",
      "Batch inference:  66%|██████▋   | 63/95 [04:26<02:20,  4.38s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 64/95 [04:30<02:15,  4.38s/it]\u001b[A\n",
      "Batch inference:  68%|██████▊   | 65/95 [04:35<02:11,  4.39s/it]\u001b[A\n",
      "Batch inference:  69%|██████▉   | 66/95 [04:39<02:07,  4.39s/it]\u001b[A\n",
      "Batch inference:  71%|███████   | 67/95 [04:43<02:02,  4.39s/it]\u001b[A\n",
      "Batch inference:  72%|███████▏  | 68/95 [04:48<01:58,  4.39s/it]\u001b[A\n",
      "Batch inference:  73%|███████▎  | 69/95 [04:52<01:54,  4.39s/it]\u001b[A\n",
      "Batch inference:  74%|███████▎  | 70/95 [04:57<01:49,  4.39s/it]\u001b[A\n",
      "Batch inference:  75%|███████▍  | 71/95 [05:01<01:45,  4.39s/it]\u001b[A\n",
      "Batch inference:  76%|███████▌  | 72/95 [05:05<01:40,  4.39s/it]\u001b[A\n",
      "Batch inference:  77%|███████▋  | 73/95 [05:10<01:36,  4.39s/it]\u001b[A\n",
      "Batch inference:  78%|███████▊  | 74/95 [05:14<01:32,  4.39s/it]\u001b[A\n",
      "Batch inference:  79%|███████▉  | 75/95 [05:18<01:27,  4.38s/it]\u001b[A\n",
      "Batch inference:  80%|████████  | 76/95 [05:23<01:23,  4.38s/it]\u001b[A\n",
      "Batch inference:  81%|████████  | 77/95 [05:27<01:18,  4.38s/it]\u001b[A\n",
      "Batch inference:  82%|████████▏ | 78/95 [05:32<01:14,  4.38s/it]\u001b[A\n",
      "Batch inference:  83%|████████▎ | 79/95 [05:36<01:10,  4.38s/it]\u001b[A\n",
      "Batch inference:  84%|████████▍ | 80/95 [05:40<01:05,  4.38s/it]\u001b[A\n",
      "Batch inference:  85%|████████▌ | 81/95 [05:45<01:01,  4.38s/it]\u001b[A\n",
      "Batch inference:  86%|████████▋ | 82/95 [05:49<00:56,  4.38s/it]\u001b[A\n",
      "Batch inference:  87%|████████▋ | 83/95 [05:54<00:52,  4.38s/it]\u001b[A\n",
      "Batch inference:  88%|████████▊ | 84/95 [05:58<00:48,  4.39s/it]\u001b[A\n",
      "Batch inference:  89%|████████▉ | 85/95 [06:02<00:43,  4.39s/it]\u001b[A\n",
      "Batch inference:  91%|█████████ | 86/95 [06:07<00:39,  4.39s/it]\u001b[A\n",
      "Batch inference:  92%|█████████▏| 87/95 [06:11<00:35,  4.39s/it]\u001b[A\n",
      "Batch inference:  93%|█████████▎| 88/95 [06:15<00:30,  4.39s/it]\u001b[A\n",
      "Batch inference:  94%|█████████▎| 89/95 [06:20<00:26,  4.39s/it]\u001b[A\n",
      "Batch inference:  95%|█████████▍| 90/95 [06:24<00:21,  4.40s/it]\u001b[A\n",
      "Batch inference:  96%|█████████▌| 91/95 [06:29<00:17,  4.40s/it]\u001b[A\n",
      "Batch inference:  97%|█████████▋| 92/95 [06:33<00:13,  4.40s/it]\u001b[A\n",
      "Batch inference:  98%|█████████▊| 93/95 [06:38<00:08,  4.40s/it]\u001b[A\n",
      "Batch inference:  99%|█████████▉| 94/95 [06:42<00:04,  4.40s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 95/95 [06:46<00:00,  4.23s/it]\u001b[A\n",
      "Processing logs:  55%|█████▍    | 205/375 [22:19<4:13:18, 89.40s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]\u001b[A\n",
      "Processing logs:  55%|█████▍    | 206/375 [22:19<3:12:46, 68.44s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\u001b[A\n",
      "Processing logs:  55%|█████▌    | 207/375 [22:19<2:23:37, 51.29s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]\u001b[A\n",
      "Processing logs:  55%|█████▌    | 208/375 [22:20<1:45:10, 37.79s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\u001b[A\n",
      "Processing logs:  56%|█████▌    | 209/375 [22:20<1:16:00, 27.47s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:00<00:02,  1.16it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 4/4 [00:07<00:00,  2.10s/it]\u001b[A\n",
      "Processing logs:  56%|█████▋    | 211/375 [22:27<46:25, 16.99s/it]  \n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processing logs:  57%|█████▋    | 213/375 [22:27<28:15, 10.47s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:04<00:00,  4.18s/it]\u001b[A\n",
      "Processing logs:  58%|█████▊    | 216/375 [22:31<16:54,  6.38s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\u001b[A\n",
      "Processing logs:  58%|█████▊    | 217/375 [22:32<14:00,  5.32s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\u001b[A\n",
      "Processing logs:  58%|█████▊    | 218/375 [22:32<11:29,  4.39s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\u001b[A\n",
      "Processing logs:  59%|█████▊    | 220/375 [22:33<07:41,  2.98s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\u001b[A\n",
      "Processing logs:  59%|█████▉    | 221/375 [22:34<06:11,  2.41s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\u001b[A\n",
      "Processing logs:  59%|█████▉    | 222/375 [22:34<04:53,  1.92s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\u001b[A\n",
      "Processing logs:  59%|█████▉    | 223/375 [22:34<03:55,  1.55s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\u001b[A\n",
      "Processing logs:  60%|█████▉    | 224/375 [22:35<03:17,  1.31s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:01<00:03,  1.69s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:06<00:03,  3.29s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:07<00:00,  2.25s/it]\u001b[A\n",
      "Processing logs:  60%|██████    | 225/375 [22:42<07:12,  2.88s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\u001b[A\n",
      "Processing logs:  60%|██████    | 226/375 [22:43<05:34,  2.24s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\u001b[A\n",
      "Processing logs:  61%|██████    | 227/375 [22:43<04:18,  1.75s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:03<00:00,  3.83s/it]\u001b[A\n",
      "Processing logs:  61%|██████    | 228/375 [22:47<05:47,  2.36s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\u001b[A\n",
      "Processing logs:  61%|██████    | 229/375 [22:47<04:19,  1.78s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\u001b[A\n",
      "Processing logs:  61%|██████▏   | 230/375 [22:48<03:20,  1.38s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\u001b[A\n",
      "Processing logs:  62%|██████▏   | 231/375 [22:48<02:32,  1.06s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\u001b[A\n",
      "Processing logs:  62%|██████▏   | 232/375 [22:49<02:02,  1.17it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\u001b[A\n",
      "Processing logs:  62%|██████▏   | 233/375 [22:49<01:36,  1.47it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\u001b[A\n",
      "Processing logs:  62%|██████▏   | 234/375 [22:49<01:24,  1.66it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.84it/s]\u001b[A\n",
      "Processing logs:  63%|██████▎   | 235/375 [22:49<01:05,  2.13it/s]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:02<00:01,  1.29s/it]\u001b[A\n",
      "Processing logs:  63%|██████▎   | 236/375 [22:52<02:29,  1.07s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\u001b[A\n",
      "Processing logs:  63%|██████▎   | 237/375 [22:52<01:48,  1.27it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\u001b[A\n",
      "Processing logs:  63%|██████▎   | 238/375 [22:52<01:23,  1.63it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  8.16it/s]\u001b[A\n",
      "Processing logs:  64%|██████▎   | 239/375 [22:52<01:03,  2.13it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\u001b[A\n",
      "Processing logs:  64%|██████▍   | 240/375 [22:53<01:00,  2.24it/s]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:01<00:00,  1.44it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]\u001b[A\n",
      "Processing logs:  64%|██████▍   | 241/375 [22:54<01:47,  1.25it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\u001b[A\n",
      "Processing logs:  65%|██████▍   | 242/375 [22:55<01:53,  1.17it/s]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:02<00:04,  2.02s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:06<00:03,  3.41s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:06<00:00,  2.05s/it]\u001b[A\n",
      "Processing logs:  65%|██████▍   | 243/375 [23:02<05:50,  2.65s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:08,  4.37s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:08<00:04,  4.42s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:10<00:00,  3.25s/it]\u001b[A\n",
      "Processing logs:  65%|██████▌   | 244/375 [23:13<11:04,  5.07s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\u001b[A\n",
      "Processing logs:  65%|██████▌   | 245/375 [23:13<08:02,  3.72s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\u001b[A\n",
      "Processing logs:  66%|██████▌   | 246/375 [23:14<06:05,  2.84s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.55it/s]\u001b[A\n",
      "Processing logs:  66%|██████▌   | 247/375 [23:14<04:20,  2.03s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\u001b[A\n",
      "Processing logs:  66%|██████▌   | 248/375 [23:15<03:26,  1.63s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\u001b[A\n",
      "Processing logs:  66%|██████▋   | 249/375 [23:15<02:38,  1.26s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:00<00:00,  2.01it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:02<00:01,  1.23s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\u001b[A\n",
      "Processing logs:  67%|██████▋   | 250/375 [23:18<03:35,  1.72s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\u001b[A\n",
      "Processing logs:  67%|██████▋   | 251/375 [23:19<02:56,  1.42s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\u001b[A\n",
      "Processing logs:  67%|██████▋   | 252/375 [23:20<02:23,  1.17s/it]\n",
      "Batch inference:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  12%|█▎        | 1/8 [00:02<00:15,  2.23s/it]\u001b[A\n",
      "Batch inference:  25%|██▌       | 2/8 [00:02<00:08,  1.34s/it]\u001b[A\n",
      "Batch inference:  38%|███▊      | 3/8 [00:03<00:04,  1.01it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 4/8 [00:04<00:03,  1.18it/s]\u001b[A\n",
      "Batch inference:  62%|██████▎   | 5/8 [00:04<00:02,  1.27it/s]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 6/8 [00:05<00:01,  1.16it/s]\u001b[A\n",
      "Batch inference:  88%|████████▊ | 7/8 [00:07<00:01,  1.06s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 8/8 [00:07<00:00,  1.27it/s]\u001b[A\n",
      "Processing logs:  67%|██████▋   | 253/375 [23:27<06:15,  3.08s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\u001b[A\n",
      "Processing logs:  68%|██████▊   | 254/375 [23:28<04:43,  2.34s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:01<00:01,  1.97s/it]\u001b[A\n",
      "Processing logs:  68%|██████▊   | 255/375 [23:30<04:30,  2.25s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\u001b[A\n",
      "Processing logs:  68%|██████▊   | 256/375 [23:31<04:07,  2.08s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]\u001b[A\n",
      "Processing logs:  69%|██████▊   | 257/375 [23:34<04:35,  2.33s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:00<00:02,  1.09it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:04<00:04,  2.43s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:07<00:02,  2.91s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 4/4 [00:09<00:00,  2.33s/it]\u001b[A\n",
      "Processing logs:  69%|██████▉   | 258/375 [23:44<08:39,  4.44s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\u001b[A\n",
      "Processing logs:  69%|██████▉   | 259/375 [23:45<06:30,  3.37s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\u001b[A\n",
      "Processing logs:  69%|██████▉   | 260/375 [23:45<04:46,  2.49s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\u001b[A\n",
      "Processing logs:  70%|██████▉   | 261/375 [23:46<03:47,  1.99s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      "Processing logs:  70%|██████▉   | 262/375 [23:46<02:48,  1.49s/it]\n",
      "Batch inference:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  17%|█▋        | 1/6 [00:01<00:06,  1.26s/it]\u001b[A\n",
      "Batch inference:  33%|███▎      | 2/6 [00:01<00:03,  1.33it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 3/6 [00:02<00:01,  1.69it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 4/6 [00:02<00:01,  1.66it/s]\u001b[A\n",
      "Batch inference:  83%|████████▎ | 5/6 [00:03<00:00,  1.79it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 6/6 [00:03<00:00,  2.21it/s]\u001b[A\n",
      "Processing logs:  70%|███████   | 263/375 [23:50<03:52,  2.07s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:00<00:01,  1.44it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:01<00:00,  1.07it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]\u001b[A\n",
      "Processing logs:  70%|███████   | 264/375 [23:52<03:46,  2.04s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\u001b[A\n",
      "Processing logs:  71%|███████   | 265/375 [23:52<02:52,  1.57s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:02<00:02,  2.20s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]\u001b[A\n",
      "Processing logs:  71%|███████   | 266/375 [23:55<03:50,  2.11s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\u001b[A\n",
      "Processing logs:  71%|███████   | 267/375 [23:56<02:52,  1.60s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.37s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:06<00:00,  2.85s/it]\u001b[A\n",
      "Processing logs:  71%|███████▏  | 268/375 [24:02<05:18,  2.98s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\u001b[A\n",
      "Processing logs:  72%|███████▏  | 269/375 [24:02<03:45,  2.12s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:00<00:01,  1.01it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:05<00:02,  2.99s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:05<00:00,  1.73s/it]\u001b[A\n",
      "Processing logs:  72%|███████▏  | 270/375 [24:08<05:33,  3.18s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:01<00:00,  1.07it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:02<00:00,  1.17it/s]\u001b[A\n",
      "Processing logs:  72%|███████▏  | 271/375 [24:10<05:11,  3.00s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:00<00:02,  1.43it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:01<00:01,  1.22it/s]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:02<00:00,  1.14it/s]\u001b[A\n",
      "Processing logs:  73%|███████▎  | 272/375 [24:13<04:57,  2.89s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\u001b[A\n",
      "Processing logs:  73%|███████▎  | 273/375 [24:15<04:42,  2.77s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\u001b[A\n",
      "Processing logs:  73%|███████▎  | 274/375 [24:17<04:06,  2.44s/it]\n",
      "Batch inference:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  11%|█         | 1/9 [00:00<00:05,  1.42it/s]\u001b[A\n",
      "Batch inference:  22%|██▏       | 2/9 [00:01<00:06,  1.13it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 3/9 [00:02<00:06,  1.00s/it]\u001b[A\n",
      "Batch inference:  44%|████▍     | 4/9 [00:04<00:06,  1.35s/it]\u001b[A\n",
      "Batch inference:  56%|█████▌    | 5/9 [00:06<00:05,  1.43s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 6/9 [00:10<00:06,  2.20s/it]\u001b[A\n",
      "Batch inference:  78%|███████▊  | 7/9 [00:11<00:04,  2.03s/it]\u001b[A\n",
      "Batch inference:  89%|████████▉ | 8/9 [00:14<00:02,  2.35s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 9/9 [00:15<00:00,  1.86s/it]\u001b[A\n",
      "Processing logs:  73%|███████▎  | 275/375 [24:33<10:37,  6.37s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\u001b[A\n",
      "Processing logs:  74%|███████▎  | 276/375 [24:33<07:31,  4.56s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\u001b[A\n",
      "Processing logs:  74%|███████▍  | 277/375 [24:33<05:18,  3.25s/it]\n",
      "Batch inference:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  14%|█▍        | 1/7 [00:00<00:04,  1.49it/s]\u001b[A\n",
      "Batch inference:  29%|██▊       | 2/7 [00:01<00:03,  1.48it/s]\u001b[A\n",
      "Batch inference:  43%|████▎     | 3/7 [00:02<00:02,  1.47it/s]\u001b[A\n",
      "Batch inference:  57%|█████▋    | 4/7 [00:02<00:02,  1.49it/s]\u001b[A\n",
      "Batch inference:  71%|███████▏  | 5/7 [00:03<00:01,  1.47it/s]\u001b[A\n",
      "Batch inference:  86%|████████▌ | 6/7 [00:04<00:00,  1.30it/s]\u001b[A\n",
      "Processing logs:  74%|███████▍  | 278/375 [24:38<05:47,  3.59s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\u001b[A\n",
      "Processing logs:  74%|███████▍  | 279/375 [24:40<05:21,  3.35s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\u001b[A\n",
      "Processing logs:  75%|███████▍  | 280/375 [24:42<04:24,  2.78s/it]\n",
      "Batch inference:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  10%|█         | 1/10 [00:00<00:06,  1.48it/s]\u001b[A\n",
      "Batch inference:  20%|██        | 2/10 [00:01<00:05,  1.40it/s]\u001b[A\n",
      "Batch inference:  30%|███       | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n",
      "Batch inference:  40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 5/10 [00:04<00:04,  1.04it/s]\u001b[A\n",
      "Batch inference:  60%|██████    | 6/10 [00:08<00:08,  2.13s/it]\u001b[A\n",
      "Batch inference:  70%|███████   | 7/10 [00:09<00:04,  1.65s/it]\u001b[A\n",
      "Batch inference:  80%|████████  | 8/10 [00:10<00:02,  1.32s/it]\u001b[A\n",
      "Batch inference:  90%|█████████ | 9/10 [00:11<00:01,  1.25s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 10/10 [00:11<00:00,  1.02it/s]\u001b[A\n",
      "Processing logs:  75%|███████▍  | 281/375 [24:54<08:32,  5.46s/it]A\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:01<00:01,  1.70s/it]\u001b[A\n",
      "Processing logs:  75%|███████▌  | 282/375 [24:55<06:44,  4.35s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\u001b[A\n",
      "Processing logs:  75%|███████▌  | 283/375 [24:56<04:53,  3.19s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\u001b[A\n",
      "Processing logs:  76%|███████▌  | 284/375 [24:57<03:50,  2.53s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:01<00:01,  1.01s/it]\u001b[A\n",
      "Processing logs:  76%|███████▌  | 285/375 [24:58<03:09,  2.10s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:00<00:01,  2.04it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:01<00:01,  1.50it/s]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]\u001b[A\n",
      "Processing logs:  76%|███████▋  | 286/375 [25:00<03:19,  2.25s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.13s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:04<00:00,  1.98s/it]\u001b[A\n",
      "Processing logs:  77%|███████▋  | 287/375 [25:05<04:20,  2.96s/it]\n",
      "Batch inference:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  17%|█▋        | 1/6 [00:00<00:02,  1.82it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 2/6 [00:01<00:03,  1.26it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 3/6 [00:02<00:02,  1.28it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 4/6 [00:03<00:01,  1.28it/s]\u001b[A\n",
      "Batch inference:  83%|████████▎ | 5/6 [00:03<00:00,  1.45it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 6/6 [00:03<00:00,  1.74it/s]\u001b[A\n",
      "Processing logs:  77%|███████▋  | 288/375 [25:09<04:43,  3.26s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\u001b[A\n",
      "Processing logs:  77%|███████▋  | 289/375 [25:09<03:26,  2.40s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\u001b[A\n",
      "Processing logs:  77%|███████▋  | 290/375 [25:10<02:44,  1.94s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:01<00:01,  1.00it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]\u001b[A\n",
      "Processing logs:  78%|███████▊  | 291/375 [25:14<03:25,  2.45s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\u001b[A\n",
      "Processing logs:  78%|███████▊  | 292/375 [25:14<02:28,  1.79s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\u001b[A\n",
      "Processing logs:  78%|███████▊  | 293/375 [25:15<01:51,  1.36s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:01<00:00,  2.05it/s]\u001b[A\n",
      "Processing logs:  78%|███████▊  | 294/375 [25:16<01:43,  1.28s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\u001b[A\n",
      "Processing logs:  79%|███████▊  | 295/375 [25:16<01:20,  1.00s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\u001b[A\n",
      "Processing logs:  79%|███████▉  | 296/375 [25:17<01:08,  1.15it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\u001b[A\n",
      "Processing logs:  79%|███████▉  | 297/375 [25:17<00:58,  1.34it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\u001b[A\n",
      "Processing logs:  79%|███████▉  | 298/375 [25:17<00:47,  1.61it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:03<00:00,  3.94s/it]\u001b[A\n",
      "Processing logs:  80%|███████▉  | 299/375 [25:21<02:03,  1.63s/it]\n",
      "Batch inference:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  20%|██        | 1/5 [00:00<00:02,  1.78it/s]\u001b[A\n",
      "Batch inference:  40%|████      | 2/5 [00:01<00:02,  1.13it/s]\u001b[A\n",
      "Batch inference:  60%|██████    | 3/5 [00:02<00:01,  1.26it/s]\u001b[A\n",
      "Batch inference:  80%|████████  | 4/5 [00:02<00:00,  1.37it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 5/5 [00:03<00:00,  1.26it/s]\u001b[A\n",
      "Processing logs:  80%|████████  | 300/375 [25:25<02:53,  2.32s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.28it/s]\u001b[A\n",
      "Processing logs:  80%|████████  | 301/375 [25:26<02:05,  1.70s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\u001b[A\n",
      "Processing logs:  81%|████████  | 302/375 [25:26<01:44,  1.43s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\u001b[A\n",
      "Processing logs:  81%|████████  | 303/375 [25:27<01:25,  1.18s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\u001b[A\n",
      "Processing logs:  81%|████████  | 304/375 [25:29<01:52,  1.59s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\u001b[A\n",
      "Processing logs:  81%|████████▏ | 305/375 [25:30<01:33,  1.33s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\u001b[A\n",
      "Processing logs:  82%|████████▏ | 306/375 [25:32<01:42,  1.49s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  7.98it/s]\u001b[A\n",
      "Processing logs:  82%|████████▏ | 307/375 [25:32<01:13,  1.08s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\u001b[A\n",
      "Processing logs:  82%|████████▏ | 308/375 [25:33<01:12,  1.08s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\u001b[A\n",
      "Processing logs:  82%|████████▏ | 309/375 [25:34<01:01,  1.08it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\u001b[A\n",
      "Processing logs:  83%|████████▎ | 310/375 [25:34<00:50,  1.30it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\u001b[A\n",
      "Processing logs:  83%|████████▎ | 311/375 [25:35<00:48,  1.32it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\u001b[A\n",
      "Processing logs:  83%|████████▎ | 312/375 [25:35<00:37,  1.70it/s]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\u001b[A\n",
      "Processing logs:  83%|████████▎ | 313/375 [25:38<01:10,  1.14s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.39s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:06<00:00,  3.04s/it]\u001b[A\n",
      "Processing logs:  84%|████████▎ | 314/375 [25:44<02:48,  2.75s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\u001b[A\n",
      "Processing logs:  84%|████████▍ | 315/375 [25:47<02:50,  2.83s/it]\n",
      "Batch inference:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  17%|█▋        | 1/6 [00:04<00:21,  4.38s/it]\u001b[A\n",
      "Batch inference:  33%|███▎      | 2/6 [00:08<00:17,  4.38s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 3/6 [00:09<00:07,  2.52s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 4/6 [00:09<00:03,  1.77s/it]\u001b[A\n",
      "Batch inference:  83%|████████▎ | 5/6 [00:10<00:01,  1.28s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 6/6 [00:10<00:00,  1.11it/s]\u001b[A\n",
      "Processing logs:  84%|████████▍ | 316/375 [25:57<04:58,  5.06s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:02<00:05,  2.94s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:07<00:03,  3.80s/it]\u001b[A\n",
      "Processing logs:  85%|████████▍ | 318/375 [26:05<04:13,  4.44s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\u001b[A\n",
      "Processing logs:  85%|████████▌ | 319/375 [26:09<04:10,  4.47s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\u001b[A\n",
      "Processing logs:  85%|████████▌ | 320/375 [26:11<03:31,  3.85s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:03<00:03,  3.64s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:07<00:00,  4.05s/it]\u001b[A\n",
      "Processing logs:  86%|████████▌ | 321/375 [26:20<04:29,  4.99s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  11%|█         | 1/9 [00:03<00:27,  3.40s/it]\u001b[A\n",
      "Batch inference:  22%|██▏       | 2/9 [00:04<00:13,  1.87s/it]\u001b[A\n",
      "Batch inference:  33%|███▎      | 3/9 [00:05<00:10,  1.82s/it]\u001b[A\n",
      "Batch inference:  44%|████▍     | 4/9 [00:07<00:09,  1.80s/it]\u001b[A\n",
      "Batch inference:  56%|█████▌    | 5/9 [00:09<00:07,  1.79s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 6/9 [00:11<00:05,  1.79s/it]\u001b[A\n",
      "Batch inference:  78%|███████▊  | 7/9 [00:13<00:03,  1.78s/it]\u001b[A\n",
      "Batch inference:  89%|████████▉ | 8/9 [00:14<00:01,  1.78s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 9/9 [00:18<00:00,  2.49s/it]\u001b[A\n",
      "Processing logs:  86%|████████▌ | 323/375 [26:39<06:02,  6.97s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\u001b[A\n",
      "Processing logs:  86%|████████▋ | 324/375 [26:40<04:53,  5.75s/it]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:04<00:08,  4.47s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:09<00:04,  4.51s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 3/3 [00:11<00:00,  3.63s/it]\u001b[A\n",
      "Processing logs:  87%|████████▋ | 325/375 [26:52<06:01,  7.24s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\u001b[A\n",
      "Processing logs:  87%|████████▋ | 327/375 [26:52<03:25,  4.27s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  6.26it/s]\u001b[A\n",
      "Processing logs:  87%|████████▋ | 328/375 [26:53<02:36,  3.33s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\u001b[A\n",
      "Processing logs:  88%|████████▊ | 329/375 [26:53<01:56,  2.53s/it]\n",
      "Batch inference:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  17%|█▋        | 1/6 [00:04<00:22,  4.50s/it]\u001b[A\n",
      "Batch inference:  33%|███▎      | 2/6 [00:09<00:18,  4.55s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 3/6 [00:13<00:13,  4.58s/it]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 4/6 [00:18<00:09,  4.59s/it]\u001b[A\n",
      "Batch inference:  83%|████████▎ | 5/6 [00:22<00:04,  4.59s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 6/6 [00:23<00:00,  3.35s/it]\u001b[A\n",
      "Processing logs:  88%|████████▊ | 330/375 [27:17<06:05,  8.11s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\u001b[A\n",
      "Processing logs:  88%|████████▊ | 331/375 [27:19<04:48,  6.56s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\u001b[A\n",
      "Processing logs:  89%|████████▊ | 332/375 [27:20<03:38,  5.08s/it]\n",
      "Batch inference:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]\u001b[A\n",
      "Batch inference:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]\u001b[A\n",
      "Batch inference:  75%|███████▌  | 3/4 [00:03<00:00,  1.08it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]\u001b[A\n",
      "Processing logs:  89%|████████▉ | 333/375 [27:24<03:20,  4.77s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\u001b[A\n",
      "Processing logs:  89%|████████▉ | 334/375 [27:25<02:32,  3.72s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\u001b[A\n",
      "Processing logs:  89%|████████▉ | 335/375 [27:26<01:52,  2.82s/it]\n",
      "Batch inference:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:   9%|▉         | 1/11 [00:02<00:23,  2.37s/it]\u001b[A\n",
      "Batch inference:  18%|█▊        | 2/11 [00:06<00:31,  3.55s/it]\u001b[A\n",
      "Batch inference:  27%|██▋       | 3/11 [00:09<00:25,  3.13s/it]\u001b[A\n",
      "Batch inference:  36%|███▋      | 4/11 [00:11<00:18,  2.70s/it]\u001b[A\n",
      "Batch inference:  45%|████▌     | 5/11 [00:13<00:15,  2.51s/it]\u001b[A\n",
      "Batch inference:  55%|█████▍    | 6/11 [00:15<00:11,  2.34s/it]\u001b[A\n",
      "Batch inference:  64%|██████▎   | 7/11 [00:17<00:08,  2.21s/it]\u001b[A\n",
      "Batch inference:  73%|███████▎  | 8/11 [00:20<00:07,  2.34s/it]\u001b[A\n",
      "Batch inference:  82%|████████▏ | 9/11 [00:23<00:04,  2.50s/it]\u001b[A\n",
      "Batch inference:  91%|█████████ | 10/11 [00:25<00:02,  2.60s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 11/11 [00:29<00:00,  3.01s/it]\u001b[A\n",
      "Processing logs:  90%|████████▉ | 336/375 [27:56<07:00, 10.79s/it]A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\u001b[A\n",
      "Processing logs:  90%|████████▉ | 337/375 [27:57<05:00,  7.91s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\u001b[A\n",
      "Processing logs:  90%|█████████ | 339/375 [27:57<02:37,  4.38s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\u001b[A\n",
      "Processing logs:  91%|█████████ | 340/375 [27:58<01:59,  3.40s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\u001b[A\n",
      "Processing logs:  91%|█████████ | 341/375 [27:58<01:30,  2.65s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\u001b[A\n",
      "Processing logs:  91%|█████████ | 342/375 [27:59<01:07,  2.05s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\u001b[A\n",
      "Processing logs:  91%|█████████▏| 343/375 [28:00<00:58,  1.84s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\u001b[A\n",
      "Processing logs:  92%|█████████▏| 344/375 [28:02<00:55,  1.80s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\u001b[A\n",
      "Processing logs:  92%|█████████▏| 345/375 [28:02<00:42,  1.43s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                      \u001b[A\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:01<00:01,  1.79s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:02<00:00,  1.13it/s]\u001b[A\n",
      "Processing logs:  93%|█████████▎| 347/375 [28:04<00:35,  1.25s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:04<00:04,  4.43s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:05<00:00,  2.28s/it]\u001b[A\n",
      "Processing logs:  93%|█████████▎| 348/375 [28:10<01:00,  2.22s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  4.62it/s]\u001b[A\n",
      "Processing logs:  93%|█████████▎| 349/375 [28:10<00:44,  1.71s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\u001b[A\n",
      "Processing logs:  93%|█████████▎| 350/375 [28:10<00:33,  1.33s/it]\n",
      "Batch inference:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]\u001b[A\n",
      "Batch inference: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]\u001b[A\n",
      "Processing logs:  94%|█████████▎| 351/375 [28:15<00:58,  2.42s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\u001b[A\n",
      "Processing logs:  94%|█████████▍| 352/375 [28:17<00:47,  2.07s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\u001b[A\n",
      "Processing logs:  94%|█████████▍| 353/375 [28:17<00:34,  1.55s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\u001b[A\n",
      "Processing logs:  94%|█████████▍| 354/375 [28:17<00:24,  1.19s/it]\n",
      "Batch inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\u001b[A\n",
      "Processing logs:  95%|█████████▍| 355/375 [28:18<00:19,  1.03it/s]\n",
      "Batch inference:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Batch inference:  33%|███▎      | 1/3 [00:00<00:00,  2.06it/s]\u001b[A\n",
      "Batch inference:  67%|██████▋   | 2/3 [00:00<00:00,  2.13it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "def classify_texts(texts: list[str]) -> list[str]:\n",
    "    preds = [\"NORMAL\"] * len(texts)               \n",
    "    non_empty = [(i, t) for i, t in enumerate(texts) if t.strip()]\n",
    "    if not non_empty:\n",
    "        return preds\n",
    "\n",
    "    idxs, to_run = zip(*non_empty)        \n",
    "    total_batches = math.ceil(len(to_run) / BATCH_SIZE)        \n",
    "    for start in tqdm(\n",
    "        range(0, len(to_run), BATCH_SIZE),\n",
    "        total=total_batches,\n",
    "        desc=\"Batch inference\",\n",
    "        leave=False,                       \n",
    "    ):\n",
    "        batch_txts = to_run[start:start + BATCH_SIZE]\n",
    "        batch = tokenizer(\n",
    "            list(batch_txts),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**batch).logits\n",
    "            batch_pred = torch.softmax(logits, dim=1).argmax(dim=1).cpu().tolist()\n",
    "\n",
    "        for pos, pred in zip(\n",
    "            idxs[start:start + BATCH_SIZE], batch_pred\n",
    "        ):\n",
    "            preds[pos] = CLASS_NAMES[pred]\n",
    "\n",
    "    return preds\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, dtype={\"log_text\": str}).fillna(\"\")\n",
    "\n",
    "errors_col, line_nums_col = [], []\n",
    "\n",
    "for raw_log in tqdm(df[\"log_text\"], desc=\"Processing logs\"):\n",
    "    lines = raw_log.splitlines()                  \n",
    "\n",
    "    explicit = [(n, l) for n, l in enumerate(lines, 1) if TAG_RE.search(l)]\n",
    "\n",
    "    if explicit:\n",
    "        err_lines  = [strip_error_tag(l) for _, l in explicit]\n",
    "        err_nums   = [str(n) for n, _ in explicit]\n",
    "\n",
    "    else:\n",
    "        preds      = classify_texts(lines)\n",
    "        err_lines  = [l for l, p in zip(lines, preds) if p == \"ERROR\"]\n",
    "        err_nums   = [str(i + 1) for i, p in enumerate(preds) if p == \"ERROR\"]\n",
    "\n",
    "    errors_col.append(\"\\n\".join(err_lines))       \n",
    "    line_nums_col.append(\",\".join(err_nums)) \n",
    "\n",
    "df[\"errors\"]              = errors_col\n",
    "df[\"errors_lines_number\"] = line_nums_col\n",
    "\n",
    "df.to_csv(CSV_OUT, index=False)\n",
    "print(f\"✔ Итог сохранён в: {CSV_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fc2db-47fa-4030-bf6e-2392ad869828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
