{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./logbert_finetuned\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForMaskedLM.from_pretrained(model_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_embeddings(logs):\n",
    "    inputs = tokenizer(logs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    cls_embeddings /= np.linalg.norm(cls_embeddings, axis=1, keepdims=True) + 1e-10\n",
    "    return cls_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlm_loss(logs):\n",
    "    inputs = tokenizer(logs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_attention = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "\n",
    "    loss_per_seq = (loss_per_token * shift_attention).sum(dim=1) / shift_attention.sum(dim=1)\n",
    "\n",
    "    return loss_per_seq.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_logs(logs, threshold):\n",
    "    losses = get_mlm_loss(logs)\n",
    "    embeddings = get_log_embeddings(logs)\n",
    "\n",
    "    classifications = []\n",
    "    for log, loss in zip(logs, losses):\n",
    "        label = \"ANOMALY\" if loss > threshold else \"NORMAL\"\n",
    "        classifications.append((log, loss, label))\n",
    "\n",
    "    return classifications, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39bd782",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = [\n",
    "    \"ERROR: failed to connect to database\",\n",
    "    \"INFO: user login successful\",\n",
    "    \"WARN: disk space low\",\n",
    "    \"CRITICAL: kernel panic occurred\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = get_mlm_loss(logs)\n",
    "threshold = np.percentile(losses, 95)\n",
    "\n",
    "results, embeddings = classify_logs(logs, threshold)\n",
    "\n",
    "for log, loss, label in results:\n",
    "    print(f\"[{label}] Loss={loss:.4f} | Log: {log}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
