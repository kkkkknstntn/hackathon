{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4038fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb18a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./logbert_finetuned\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForMaskedLM.from_pretrained(model_dir)\n",
    "model.eval()\n",
    "\n",
    "def get_log_embeddings(logs):\n",
    "    inputs = tokenizer(logs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    cls_embeddings /= np.linalg.norm(cls_embeddings, axis=1, keepdims=True) + 1e-10\n",
    "    return cls_embeddings\n",
    "\n",
    "logs = [\n",
    "    \"ERROR: failed to connect to database\",\n",
    "    \"INFO: user login successful\",\n",
    "    \"WARN: disk space low\",\n",
    "]\n",
    "\n",
    "embeddings = get_log_embeddings(logs)\n",
    "print(\"Shape embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlm_loss(logs):\n",
    "    inputs = tokenizer(logs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_attention = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "    \n",
    "    loss_per_seq = (loss_per_token * shift_attention).sum(dim=1) / shift_attention.sum(dim=1)\n",
    "    \n",
    "    return loss_per_seq.cpu().numpy()\n",
    "\n",
    "losses = get_mlm_loss(logs)\n",
    "for log, loss in zip(logs, losses):\n",
    "    print(f\"Loss: {loss:.4f} | Log: {log}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".error-hack-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
