{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65bb7df",
   "metadata": {},
   "source": [
    "## Пресет, делает эмбеддинги по мешку, LLM, тфидф строит фаисс индекс для поиска потом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94021b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device=None):\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def embed(self, texts):\n",
    "        # texts - список строк\n",
    "        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        \n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        attention_mask = encoded_input['attention_mask'].unsqueeze(-1)\n",
    "        \n",
    "        summed = torch.sum(token_embeddings * attention_mask, dim=1)\n",
    "        counts = torch.clamp(attention_mask.sum(dim=1), min=1e-9)\n",
    "        mean_pooled = summed / counts\n",
    "        \n",
    "        embeddings = mean_pooled.cpu().numpy()\n",
    "        embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebf333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWordsEmbedder:\n",
    "    def __init__(self, max_features=10000):\n",
    "        self.vectorizer = CountVectorizer(max_features=max_features)\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, texts):\n",
    "        self.vectorizer.fit(texts)\n",
    "        self.fitted = True\n",
    "\n",
    "    def embed(self, texts):\n",
    "        if not self.fitted:\n",
    "            self.fit(texts)\n",
    "        vectors = self.vectorizer.transform(texts).toarray().astype(np.float32)\n",
    "        vectors /= np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbedder:\n",
    "    def __init__(self, max_features=10000):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, texts):\n",
    "        self.vectorizer.fit(texts)\n",
    "        self.fitted = True\n",
    "\n",
    "    def embed(self, texts):\n",
    "        if not self.fitted:\n",
    "            self.fit(texts)\n",
    "        vectors = self.vectorizer.transform(texts).toarray().astype(np.float32)\n",
    "        vectors /= np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33659903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissIndex:\n",
    "    def __init__(self, dimension):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    def add(self, embeddings):\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "    \n",
    "    def save(self, path):\n",
    "        faiss.write_index(self.index, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.index = faiss.read_index(path)\n",
    "    \n",
    "    def search(self, query_embeddings, top_k=5):\n",
    "        D, I = self.index.search(query_embeddings.astype(np.float32), top_k)\n",
    "        return D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94663a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "        \"Hello world\",\n",
    "        \"Hi there\",\n",
    "        \"Goodbye world\",\n",
    "        \"Hello from the other side\",\n",
    "        \"I love machine learning\",\n",
    "        \"Transformers are amazing\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder()\n",
    "embeddings = embedder.embed(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = FaissIndex(dimension=embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "index.save(\"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = FaissIndex(dimension=embeddings.shape[1])\n",
    "index2.load(\"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c52c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"I enjoy machine learning and AI\"]\n",
    "query_emb = embedder.embed(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = index2.search(query_emb, top_k=3)\n",
    "print(\"Top matches:\")\n",
    "for dist, idx in zip(distances[0], indices[0]):\n",
    "  print(f\"Text: {texts[idx]} | Score (cosine sim): {dist:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
