{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58452185-afed-4089-901a-97d73201ce8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:42:00.837266Z",
     "iopub.status.busy": "2025-05-16T22:42:00.836512Z",
     "iopub.status.idle": "2025-05-16T22:42:00.868437Z",
     "shell.execute_reply": "2025-05-16T22:42:00.867336Z",
     "shell.execute_reply.started": "2025-05-16T22:42:00.837230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74372ac9-f875-49ed-b1e2-47a1bb302dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:42:00.871058Z",
     "iopub.status.busy": "2025-05-16T22:42:00.870086Z",
     "iopub.status.idle": "2025-05-16T22:42:00.884183Z",
     "shell.execute_reply": "2025-05-16T22:42:00.882943Z",
     "shell.execute_reply.started": "2025-05-16T22:42:00.871005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5ae159-1530-44fd-a8eb-7dc4cbae3893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:49:49.084002Z",
     "iopub.status.busy": "2025-05-16T22:49:49.082396Z",
     "iopub.status.idle": "2025-05-16T22:52:53.018976Z",
     "shell.execute_reply": "2025-05-16T22:52:53.017649Z",
     "shell.execute_reply.started": "2025-05-16T22:49:49.083940Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего ошибок: 375\n",
      "TF-IDF shape: (375, 5000)\n",
      "⏳ all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [00:02<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [00:04<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ bge-base-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:25<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:26<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ distil-log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:24<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ deberta-v3-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:09<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== all-MiniLM-L6-v2 ===\n",
      "kmeans   [params: {'n_clusters': 20, 'n_init': 'auto'}] → clusters=20\n",
      "kmeans   [params: {'n_clusters': 30, 'n_init': 10}] → clusters=30\n",
      "kmeans   [params: {'n_clusters': 5, 'n_init': 10}] → clusters=5\n",
      "agglo    [params: {'n_clusters': 20, 'linkage': 'ward'}] → clusters=20\n",
      "agglo    [params: {'n_clusters': 15, 'linkage': 'complete'}] → clusters=15\n",
      "dbscan   [params: {'eps': 0.3, 'min_samples': 10, 'metric': 'euclidean'}] → clusters=1\n",
      "dbscan   [params: {'eps': 0.5, 'min_samples': 15, 'metric': 'cosine'}] → clusters=2\n",
      "hdbscan  [params: {'min_cluster_size': 5, 'metric': 'manhattan'}] → clusters=4\n",
      "hdbscan  [params: {'min_cluster_size': 10, 'metric': 'euclidean'}] → clusters=3\n",
      "\n",
      "=== all-mpnet-base-v2 ===\n",
      "kmeans   [params: {'n_clusters': 20, 'n_init': 'auto'}] → clusters=20\n",
      "kmeans   [params: {'n_clusters': 30, 'n_init': 10}] → clusters=30\n",
      "kmeans   [params: {'n_clusters': 5, 'n_init': 10}] → clusters=5\n",
      "agglo    [params: {'n_clusters': 20, 'linkage': 'ward'}] → clusters=20\n",
      "agglo    [params: {'n_clusters': 15, 'linkage': 'complete'}] → clusters=15\n",
      "dbscan   [params: {'eps': 0.3, 'min_samples': 10, 'metric': 'euclidean'}] → clusters=1\n",
      "dbscan   [params: {'eps': 0.5, 'min_samples': 15, 'metric': 'cosine'}] → clusters=2\n",
      "hdbscan  [params: {'min_cluster_size': 5, 'metric': 'manhattan'}] → clusters=4\n",
      "hdbscan  [params: {'min_cluster_size': 10, 'metric': 'euclidean'}] → clusters=4\n",
      "\n",
      "=== bge-base-en-v1.5 ===\n",
      "kmeans   [params: {'n_clusters': 20, 'n_init': 'auto'}] → clusters=20\n",
      "kmeans   [params: {'n_clusters': 30, 'n_init': 10}] → clusters=30\n",
      "kmeans   [params: {'n_clusters': 5, 'n_init': 10}] → clusters=5\n",
      "agglo    [params: {'n_clusters': 20, 'linkage': 'ward'}] → clusters=20\n",
      "agglo    [params: {'n_clusters': 15, 'linkage': 'complete'}] → clusters=15\n",
      "dbscan   [params: {'eps': 0.3, 'min_samples': 10, 'metric': 'euclidean'}] → clusters=6\n",
      "dbscan   [params: {'eps': 0.5, 'min_samples': 15, 'metric': 'cosine'}] → clusters=1\n",
      "hdbscan  [params: {'min_cluster_size': 5, 'metric': 'manhattan'}] → clusters=13\n",
      "hdbscan  [params: {'min_cluster_size': 10, 'metric': 'euclidean'}] → clusters=3\n",
      "\n",
      "=== bge-large-en-v1.5 ===\n",
      "kmeans   [params: {'n_clusters': 20, 'n_init': 'auto'}] → clusters=20\n",
      "kmeans   [params: {'n_clusters': 30, 'n_init': 10}] → clusters=30\n",
      "kmeans   [params: {'n_clusters': 5, 'n_init': 10}] → clusters=5\n",
      "agglo    [params: {'n_clusters': 20, 'linkage': 'ward'}] → clusters=20\n",
      "agglo    [params: {'n_clusters': 15, 'linkage': 'complete'}] → clusters=15\n",
      "dbscan   [params: {'eps': 0.3, 'min_samples': 10, 'metric': 'euclidean'}] → clusters=6\n",
      "dbscan   [params: {'eps': 0.5, 'min_samples': 15, 'metric': 'cosine'}] → clusters=1\n",
      "hdbscan  [params: {'min_cluster_size': 5, 'metric': 'manhattan'}] → clusters=12\n",
      "hdbscan  [params: {'min_cluster_size': 10, 'metric': 'euclidean'}] → clusters=3\n",
      "\n",
      "=== deberta-v3-large ===\n",
      "kmeans   [params: {'n_clusters': 20, 'n_init': 'auto'}] → clusters=20\n",
      "kmeans   [params: {'n_clusters': 30, 'n_init': 10}] → clusters=30\n",
      "kmeans   [params: {'n_clusters': 5, 'n_init': 10}] → clusters=5\n",
      "agglo    [params: {'n_clusters': 20, 'linkage': 'ward'}] → clusters=20\n",
      "agglo    [params: {'n_clusters': 15, 'linkage': 'complete'}] → clusters=15\n",
      "dbscan   [params: {'eps': 0.3, 'min_samples': 10, 'metric': 'euclidean'}] → clusters=2\n",
      "dbscan   [params: {'eps': 0.5, 'min_samples': 15, 'metric': 'cosine'}] → clusters=1\n",
      "hdbscan  [params: {'min_cluster_size': 5, 'metric': 'manhattan'}] → clusters=2\n",
      "hdbscan  [params: {'min_cluster_size': 10, 'metric': 'euclidean'}] → clusters=4\n",
      "\n",
      "=== distil-log ===\n",
      "kmeans   [params: {'n_clusters': 20, 'n_init': 'auto'}] → clusters=20\n",
      "kmeans   [params: {'n_clusters': 30, 'n_init': 10}] → clusters=30\n",
      "kmeans   [params: {'n_clusters': 5, 'n_init': 10}] → clusters=5\n",
      "agglo    [params: {'n_clusters': 20, 'linkage': 'ward'}] → clusters=20\n",
      "agglo    [params: {'n_clusters': 15, 'linkage': 'complete'}] → clusters=15\n",
      "dbscan   [params: {'eps': 0.3, 'min_samples': 10, 'metric': 'euclidean'}] → clusters=1\n",
      "dbscan   [params: {'eps': 0.5, 'min_samples': 15, 'metric': 'cosine'}] → clusters=1\n",
      "hdbscan  [params: {'min_cluster_size': 5, 'metric': 'manhattan'}] → clusters=11\n",
      "hdbscan  [params: {'min_cluster_size': 10, 'metric': 'euclidean'}] → clusters=3\n",
      "\n",
      "=== roberta-large ===\n",
      "kmeans   [params: {'n_clusters': 20, 'n_init': 'auto'}] → clusters=20\n",
      "kmeans   [params: {'n_clusters': 30, 'n_init': 10}] → clusters=30\n",
      "kmeans   [params: {'n_clusters': 5, 'n_init': 10}] → clusters=5\n",
      "agglo    [params: {'n_clusters': 20, 'linkage': 'ward'}] → clusters=20\n",
      "agglo    [params: {'n_clusters': 15, 'linkage': 'complete'}] → clusters=15\n",
      "dbscan   [params: {'eps': 0.3, 'min_samples': 10, 'metric': 'euclidean'}] → clusters=1\n",
      "dbscan   [params: {'eps': 0.5, 'min_samples': 15, 'metric': 'cosine'}] → clusters=1\n",
      "hdbscan  [params: {'min_cluster_size': 5, 'metric': 'manhattan'}] → clusters=3\n",
      "hdbscan  [params: {'min_cluster_size': 10, 'metric': 'euclidean'}] → clusters=3\n",
      "\n",
      "=== tfidf ===\n",
      "kmeans   [params: {'n_clusters': 20, 'n_init': 'auto'}] → clusters=20\n",
      "kmeans   [params: {'n_clusters': 30, 'n_init': 10}] → clusters=30\n",
      "kmeans   [params: {'n_clusters': 5, 'n_init': 10}] → clusters=5\n",
      "agglo    [params: {'n_clusters': 20, 'linkage': 'ward'}] → clusters=20\n",
      "agglo    [params: {'n_clusters': 15, 'linkage': 'complete'}] → clusters=15\n",
      "dbscan   [params: {'eps': 0.3, 'min_samples': 10, 'metric': 'euclidean'}] → clusters=2\n",
      "dbscan   [params: {'eps': 0.5, 'min_samples': 15, 'metric': 'cosine'}] → clusters=4\n",
      "hdbscan  [params: {'min_cluster_size': 5, 'metric': 'manhattan'}] → clusters=7\n",
      "hdbscan  [params: {'min_cluster_size': 10, 'metric': 'euclidean'}] → clusters=6\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_agglo_n_clusters=15_linkage=complete_labels.md\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_agglo_n_clusters=20_linkage=ward_labels.md\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_dbscan_eps=0.3_min_samples=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_dbscan_eps=0.5_min_samples=15_metric=cosine_labels.md\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_hdbscan_min_cluster_size=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_hdbscan_min_cluster_size=5_metric=manhattan_labels.md\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_kmeans_n_clusters=20_n_init=auto_labels.md\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_kmeans_n_clusters=30_n_init=10_labels.md\n",
      "✓ cluster_objects/all-MiniLM-L6-v2_kmeans_n_clusters=5_n_init=10_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_agglo_n_clusters=15_linkage=complete_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_agglo_n_clusters=20_linkage=ward_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_dbscan_eps=0.3_min_samples=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_dbscan_eps=0.5_min_samples=15_metric=cosine_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_hdbscan_min_cluster_size=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_hdbscan_min_cluster_size=5_metric=manhattan_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_kmeans_n_clusters=20_n_init=auto_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_kmeans_n_clusters=30_n_init=10_labels.md\n",
      "✓ cluster_objects/all-mpnet-base-v2_kmeans_n_clusters=5_n_init=10_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_agglo_n_clusters=15_linkage=complete_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_agglo_n_clusters=20_linkage=ward_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_dbscan_eps=0.3_min_samples=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_dbscan_eps=0.5_min_samples=15_metric=cosine_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_hdbscan_min_cluster_size=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_hdbscan_min_cluster_size=5_metric=manhattan_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_kmeans_n_clusters=20_n_init=auto_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_kmeans_n_clusters=30_n_init=10_labels.md\n",
      "✓ cluster_objects/bge-base-en-v1.5_kmeans_n_clusters=5_n_init=10_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_agglo_n_clusters=15_linkage=complete_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_agglo_n_clusters=20_linkage=ward_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_dbscan_eps=0.3_min_samples=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_dbscan_eps=0.5_min_samples=15_metric=cosine_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_hdbscan_min_cluster_size=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_hdbscan_min_cluster_size=5_metric=manhattan_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_kmeans_n_clusters=20_n_init=auto_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_kmeans_n_clusters=30_n_init=10_labels.md\n",
      "✓ cluster_objects/bge-large-en-v1.5_kmeans_n_clusters=5_n_init=10_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_agglo_n_clusters=15_linkage=complete_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_agglo_n_clusters=20_linkage=ward_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_dbscan_eps=0.3_min_samples=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_dbscan_eps=0.5_min_samples=15_metric=cosine_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_hdbscan_min_cluster_size=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_hdbscan_min_cluster_size=5_metric=manhattan_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_kmeans_n_clusters=20_n_init=auto_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_kmeans_n_clusters=30_n_init=10_labels.md\n",
      "✓ cluster_objects/deberta-v3-large_kmeans_n_clusters=5_n_init=10_labels.md\n",
      "✓ cluster_objects/distil-log_agglo_n_clusters=15_linkage=complete_labels.md\n",
      "✓ cluster_objects/distil-log_agglo_n_clusters=20_linkage=ward_labels.md\n",
      "✓ cluster_objects/distil-log_dbscan_eps=0.3_min_samples=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/distil-log_dbscan_eps=0.5_min_samples=15_metric=cosine_labels.md\n",
      "✓ cluster_objects/distil-log_hdbscan_min_cluster_size=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/distil-log_hdbscan_min_cluster_size=5_metric=manhattan_labels.md\n",
      "✓ cluster_objects/distil-log_kmeans_n_clusters=20_n_init=auto_labels.md\n",
      "✓ cluster_objects/distil-log_kmeans_n_clusters=30_n_init=10_labels.md\n",
      "✓ cluster_objects/distil-log_kmeans_n_clusters=5_n_init=10_labels.md\n",
      "✓ cluster_objects/roberta-large_agglo_n_clusters=15_linkage=complete_labels.md\n",
      "✓ cluster_objects/roberta-large_agglo_n_clusters=20_linkage=ward_labels.md\n",
      "✓ cluster_objects/roberta-large_dbscan_eps=0.3_min_samples=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/roberta-large_dbscan_eps=0.5_min_samples=15_metric=cosine_labels.md\n",
      "✓ cluster_objects/roberta-large_hdbscan_min_cluster_size=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/roberta-large_hdbscan_min_cluster_size=5_metric=manhattan_labels.md\n",
      "✓ cluster_objects/roberta-large_kmeans_n_clusters=20_n_init=auto_labels.md\n",
      "✓ cluster_objects/roberta-large_kmeans_n_clusters=30_n_init=10_labels.md\n",
      "✓ cluster_objects/roberta-large_kmeans_n_clusters=5_n_init=10_labels.md\n",
      "✓ cluster_objects/tfidf_agglo_n_clusters=15_linkage=complete_labels.md\n",
      "✓ cluster_objects/tfidf_agglo_n_clusters=20_linkage=ward_labels.md\n",
      "✓ cluster_objects/tfidf_dbscan_eps=0.3_min_samples=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/tfidf_dbscan_eps=0.5_min_samples=15_metric=cosine_labels.md\n",
      "✓ cluster_objects/tfidf_hdbscan_min_cluster_size=10_metric=euclidean_labels.md\n",
      "✓ cluster_objects/tfidf_hdbscan_min_cluster_size=5_metric=manhattan_labels.md\n",
      "✓ cluster_objects/tfidf_kmeans_n_clusters=20_n_init=auto_labels.md\n",
      "✓ cluster_objects/tfidf_kmeans_n_clusters=30_n_init=10_labels.md\n",
      "✓ cluster_objects/tfidf_kmeans_n_clusters=5_n_init=10_labels.md\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>noise_points</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>davies_bouldin</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137729</td>\n",
       "      <td>2.082332</td>\n",
       "      <td>n_clusters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137729</td>\n",
       "      <td>2.082332</td>\n",
       "      <td>n_init</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127542</td>\n",
       "      <td>2.079223</td>\n",
       "      <td>n_clusters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127542</td>\n",
       "      <td>2.079223</td>\n",
       "      <td>n_init</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146215</td>\n",
       "      <td>2.098053</td>\n",
       "      <td>n_clusters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146215</td>\n",
       "      <td>2.098053</td>\n",
       "      <td>n_init</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>agglo</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.149139</td>\n",
       "      <td>2.194724</td>\n",
       "      <td>n_clusters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>agglo</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.149139</td>\n",
       "      <td>2.194724</td>\n",
       "      <td>linkage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>agglo</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.128809</td>\n",
       "      <td>1.974630</td>\n",
       "      <td>n_clusters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>agglo</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.128809</td>\n",
       "      <td>1.974630</td>\n",
       "      <td>linkage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          embedding algorithm  ...  davies_bouldin      params\n",
       "0  all-MiniLM-L6-v2    kmeans  ...        2.082332  n_clusters\n",
       "1  all-MiniLM-L6-v2    kmeans  ...        2.082332      n_init\n",
       "2  all-MiniLM-L6-v2    kmeans  ...        2.079223  n_clusters\n",
       "3  all-MiniLM-L6-v2    kmeans  ...        2.079223      n_init\n",
       "4  all-MiniLM-L6-v2    kmeans  ...        2.098053  n_clusters\n",
       "5  all-MiniLM-L6-v2    kmeans  ...        2.098053      n_init\n",
       "6  all-MiniLM-L6-v2     agglo  ...        2.194724  n_clusters\n",
       "7  all-MiniLM-L6-v2     agglo  ...        2.194724     linkage\n",
       "8  all-MiniLM-L6-v2     agglo  ...        1.974630  n_clusters\n",
       "9  all-MiniLM-L6-v2     agglo  ...        1.974630     linkage\n",
       "\n",
       "[10 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import os, json, math, random, pathlib, textwrap, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.cluster import (KMeans, DBSCAN, Birch, AgglomerativeClustering,\n",
    "                             OPTICS, SpectralClustering)\n",
    "\n",
    "from umap import UMAP  # Исправление импорта для UMAP\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# %%\n",
    "DATA_PATH = \"../logs_with_labels.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['id'] = df.index\n",
    "errors_texts = df['errors'].astype(str).tolist()\n",
    "print(f\"Всего ошибок: {len(errors_texts):,}\")\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "EMB_DIR = pathlib.Path(\"embeddings\"); EMB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODELS = {\n",
    "    \"tfidf\": None,\n",
    "    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"all-mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"bge-base-en-v1.5\": \"BAAI/bge-base-en-v1.5\",\n",
    "    \"bge-large-en-v1.5\": \"BAAI/bge-large-en-v1.5\",\n",
    "    \"distil-log\": \"teoogherghi/Log-Analysis-Model-DistilBert\",\n",
    "    \"roberta-large\": \"roberta-large\",\n",
    "    \"deberta-v3-large\": \"microsoft/deberta-v3-large\"\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH = 32  # Уменьшенный размер батча для больших моделей\n",
    "\n",
    "# %%\n",
    "def encode_st(model_name: str, texts, batch_size=BATCH):\n",
    "    encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "    emb = encoder.encode(texts, batch_size=batch_size,\n",
    "                         show_progress_bar=True,\n",
    "                         convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return emb.astype(np.float32)\n",
    "\n",
    "# %%\n",
    "tfidf_vec = TfidfVectorizer(max_features=5000,\n",
    "                            token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
    "tfidf_mat = tfidf_vec.fit_transform(errors_texts)\n",
    "tfidf_df = pd.DataFrame(tfidf_mat.toarray(), index=df.id)\n",
    "tfidf_df.to_csv(EMB_DIR / \"tfidf.csv\", index_label=\"id\")\n",
    "print(\"TF-IDF shape:\", tfidf_mat.shape)\n",
    "\n",
    "# %%\n",
    "# Установите необходимые зависимости если еще не установлены\n",
    "# !pip install sentencepiece transformers>=4.40.0\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    RobertaTokenizer,       # Для RoBERTa\n",
    "    DebertaV2Tokenizer      # Для DeBERTa-v3\n",
    ")\n",
    "\n",
    "\n",
    "for key, hub_id in MODELS.items():\n",
    "    if key == \"tfidf\":\n",
    "        continue\n",
    "    print(f\"⏳ {key}\")\n",
    "    if key in [\"distil-log\", \"bge-base-en-v1.5\", \"bge-large-en-v1.5\", \"roberta-large\", \"deberta-v3-large\"]:\n",
    "        # Явно выбираем токенизатор для проблемных моделей\n",
    "        if key == \"deberta-v3-large\":\n",
    "            tok = DebertaV2Tokenizer.from_pretrained(hub_id)\n",
    "        elif key == \"roberta-large\":\n",
    "            tok = RobertaTokenizer.from_pretrained(hub_id)\n",
    "        else:\n",
    "            tok = AutoTokenizer.from_pretrained(hub_id, use_fast=False)\n",
    "        \n",
    "        mdl = AutoModel.from_pretrained(hub_id).to(DEVICE).eval()\n",
    "        \n",
    "        # Для DeBERTa и RoBERTa добавляем специальные параметры\n",
    "        tokenizer_kwargs = {\n",
    "            'truncation': True,\n",
    "            'max_length': 128,\n",
    "            'padding': 'max_length' if key == \"deberta-v3-large\" else True,\n",
    "            'return_tensors': \"pt\"\n",
    "        }\n",
    "        \n",
    "        all_vecs = []\n",
    "        for i in tqdm(range(0, len(errors_texts), BATCH)):\n",
    "            batch = errors_texts[i:i+BATCH]\n",
    "            inputs = tok(batch, **tokenizer_kwargs).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outs = mdl(**inputs).last_hidden_state.mean(1)\n",
    "            all_vecs.append(outs.cpu())\n",
    "        emb = torch.cat(all_vecs, 0).numpy()\n",
    "        emb = normalize(emb)\n",
    "    else:\n",
    "        emb = encode_st(hub_id, errors_texts)\n",
    "    pd.DataFrame(emb, index=df.id).to_csv(EMB_DIR / f\"{key}.csv\", index_label=\"id\")\n",
    "\n",
    "# %%\n",
    "CLUSTER_DIR = pathlib.Path(\"clusters\"); CLUSTER_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "ALGORITHMS = {\n",
    "    \"kmeans\": {\n",
    "        \"func\": lambda X, params: KMeans(**params).fit(X).labels_,\n",
    "        \"params\": [\n",
    "            {\"n_clusters\": 20, \"n_init\": \"auto\"},\n",
    "            {\"n_clusters\": 30, \"n_init\": 10},\n",
    "            {\"n_clusters\": 5, \"n_init\": 10}\n",
    "        ]\n",
    "    },\n",
    "    \"agglo\": {\n",
    "        \"func\": lambda X, params: AgglomerativeClustering(**params).fit_predict(X),\n",
    "        \"params\": [\n",
    "            {\"n_clusters\": 20, \"linkage\": \"ward\"},\n",
    "            {\"n_clusters\": 15, \"linkage\": \"complete\"}\n",
    "        ]\n",
    "    },\n",
    "    \"dbscan\": {\n",
    "        \"func\": lambda X, params: DBSCAN(**params).fit_predict(X),\n",
    "        \"params\": [\n",
    "            {\"eps\": 0.3, \"min_samples\": 10, \"metric\": \"euclidean\"},\n",
    "            {\"eps\": 0.5, \"min_samples\": 15, \"metric\": \"cosine\"}\n",
    "        ]\n",
    "    },\n",
    "    \"hdbscan\": {\n",
    "        \"func\": lambda X, params: hdbscan.HDBSCAN(**params).fit_predict(X),\n",
    "        \"params\": [\n",
    "            {\"min_cluster_size\": 5, \"metric\": \"manhattan\"},\n",
    "            {\"min_cluster_size\": 10, \"metric\": \"euclidean\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# %%\n",
    "def cluster_and_report(name, X, algo_key, params):\n",
    "    algo_func = ALGORITHMS[algo_key][\"func\"]\n",
    "    try:\n",
    "        labels = algo_func(X, params)\n",
    "        n_clusters = len(set(labels))\n",
    "        \n",
    "        stats = {\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"noise_points\": np.sum(labels == -1),\n",
    "            \"silhouette\": silhouette_score(X, labels) if n_clusters > 1 else np.nan,\n",
    "            \"davies_bouldin\": davies_bouldin_score(X, labels) if n_clusters > 1 else np.nan\n",
    "        }\n",
    "        \n",
    "        label_series = pd.Series(labels, index=df.id, name=\"cluster\")\n",
    "        label_series.to_csv(CLUSTER_DIR / f\"{name}_{algo_key}_{param_str(params)}_labels.csv\", index_label=\"id\")\n",
    "        \n",
    "        return {**stats, \"params\": params}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ {algo_key} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def param_str(params):\n",
    "    return \"_\".join(f\"{k}={v}\" for k,v in params.items())\n",
    "\n",
    "# %%\n",
    "results = []\n",
    "\n",
    "for emb_file in sorted(EMB_DIR.glob(\"*.csv\")):\n",
    "    name = emb_file.stem\n",
    "    X = pd.read_csv(emb_file, index_col=\"id\").values\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    \n",
    "    for algo_key in ALGORITHMS:\n",
    "        for params in ALGORITHMS[algo_key][\"params\"]:\n",
    "            result = cluster_and_report(name, X, algo_key, params)\n",
    "            if result:\n",
    "                results.append({\n",
    "                    \"embedding\": name,\n",
    "                    \"algorithm\": algo_key,\n",
    "                    **result\n",
    "                })\n",
    "                print(f\"{algo_key:8s} [params: {params}] → clusters={result['n_clusters']}\")\n",
    "\n",
    "# %%\n",
    "CENTRAL_DIR = pathlib.Path(\"cluster_objects\"); CENTRAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def central_extremes(X, labels, top_k=10):\n",
    "    out = defaultdict(lambda: {\"central\": [], \"extreme\": []})\n",
    "    for c in set(labels):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        part = X[idx]\n",
    "        center = part.mean(0, keepdims=True)\n",
    "        dists = np.linalg.norm(part - center, axis=1)\n",
    "        order = np.argsort(dists)\n",
    "        out[c][\"central\"] = idx[order[:top_k]].tolist()\n",
    "        out[c][\"extreme\"] = idx[order[::-1][:top_k]].tolist()\n",
    "    return out\n",
    "\n",
    "# %%\n",
    "for lab_file in sorted(CLUSTER_DIR.glob(\"*_labels.csv\")):\n",
    "    parts = lab_file.stem.split(\"_\")\n",
    "    name, algo = parts[0], \"_\".join(parts[1:-1])\n",
    "    labels = pd.read_csv(lab_file, index_col=\"id\").squeeze().values\n",
    "    X = pd.read_csv(EMB_DIR / f\"{name}.csv\", index_col=\"id\").values\n",
    "    ce = central_extremes(X, labels)\n",
    "\n",
    "    md_path = CENTRAL_DIR / f\"{lab_file.stem}.md\"\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(f\"# {name} + {algo}\\n\\n\")\n",
    "        fp.write(f\"## Cluster Statistics\\n\")\n",
    "        fp.write(f\"- Total clusters: {len(ce)}\\n\")\n",
    "        fp.write(f\"- Noise points: {np.sum(labels == -1)}\\n\\n\")\n",
    "        \n",
    "        for c, data in ce.items():\n",
    "            fp.write(f\"## Cluster {c} (Size: {len(data['central']) + len(data['extreme'])})\\n\\n\")\n",
    "            fp.write(\"### Central objects\\n\")\n",
    "            for ix in data[\"central\"]:\n",
    "                row = df.loc[ix]\n",
    "                fp.write(f\"* **id={ix}** — `{row.errors}`\\n\")  # Полный текст\n",
    "            fp.write(\"\\n### Extreme objects\\n\")\n",
    "            for ix in data[\"extreme\"]:\n",
    "                row = df.loc[ix]\n",
    "                fp.write(f\"* **id={ix}** — `{row.errors}`\\n\")  # Полный текст\n",
    "            fp.write(\"\\n---\\n\")\n",
    "    print(\"✓\", md_path)\n",
    "\n",
    "# %%\n",
    "VIS_DIR = pathlib.Path(\"viz\"); VIS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def umap_plot(name, algo, X, labels):\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, \n",
    "                       n_neighbors=15, min_dist=0.1, metric='cosine')\n",
    "    emb2d = reducer.fit_transform(X)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    scatter = plt.scatter(emb2d[:,0], emb2d[:,1], s=10, c=labels, \n",
    "                         cmap='tab20', alpha=0.8)\n",
    "    plt.title(f\"{name} + {algo}\\nClusters: {len(set(labels))}\")\n",
    "    plt.colorbar(scatter)\n",
    "    path = VIS_DIR / f\"{name}_{algo}.png\"\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "    return path\n",
    "\n",
    "# %%\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df = res_df.explode(\"params\").reset_index(drop=True)\n",
    "res_df.to_markdown(\"full_report.md\", index=False)\n",
    "res_df.to_csv(\"full_report.csv\", index=False)\n",
    "res_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
